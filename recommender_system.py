# [file name]: recommender_system.py
import torch
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
import networkx as nx
from py2neo import Graph
import dgl
import json
import os
from typing import List, Dict, Tuple, Any
from sentence_transformers import SentenceTransformer
import heapq
from collections import defaultdict
import warnings
from datetime import datetime
warnings.filterwarnings('ignore')

class AcademicRecommender:
    def __init__(self, 
                 model_path=r"training\models\trial5\han_embeddings.pth", 
                 neo4j_uri="neo4j://127.0.0.1:7687",
                 neo4j_username="neo4j",
                 neo4j_password="87654321"):
        """åˆå§‹åŒ–å­¦æœ¯æ¨èç³»ç»Ÿ"""
        self.device = torch.device('cuda')
        
        # åŠ è½½è®­ç»ƒå¥½çš„åµŒå…¥ï¼ˆå°è£…åˆ° helperï¼Œæ”¯æŒå®¹é”™ä¸ç¼“å­˜ï¼‰
        self._emb_np_cache = {}
        print("ğŸ“¥ Loading trained embeddings...")
        try:
            self._load_checkpoint(model_path)
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            raise
        
        # è¿æ¥Neo4jè·å–å…ƒæ•°æ®
        print("ğŸ”Œ Connecting to Neo4j...")
        try:
            self.graph_db = Graph(neo4j_uri, auth=(neo4j_username, neo4j_password))
            # æµ‹è¯•è¿æ¥
            self.graph_db.run("RETURN 1")
            print("âœ… Neo4j connection successful")
        except Exception as e:
            print(f"âŒ Failed to connect to Neo4j: {e}")
            # åˆ›å»ºç©ºçš„å›¾æ•°æ®åº“è¿æ¥ï¼Œä½†æ ‡è®°ä¸ºä¸å¯ç”¨
            self.graph_db = None
        
        # åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨
        try:
            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
            print("âœ… Sentence transformer loaded")
        except Exception as e:
            print(f"âŒ Failed to load sentence transformer: {e}")
            self.sentence_model = None
        
        # æ„å»ºåå‘æ˜ å°„
        self.reverse_maps = {}
        for node_type, id_map in self.id_maps.items():
            self.reverse_maps[node_type] = {v: k for k, v in id_map.items()}
        
        print("âœ… Recommender system initialized!")

    def _load_checkpoint(self, model_path: str):
        """åŠ è½½ checkpointï¼Œå¹¶å‡†å¤‡ embeddings çš„ numpy ç¼“å­˜"""
        # æ”¯æŒå¤šç§ç›¸å¯¹/ç»å¯¹è·¯å¾„å°è¯•
        tried = []
        paths_to_try = [model_path]
        # å¸¸è§å¯èƒ½çš„ç›¸å¯¹ä½ç½®
        paths_to_try.extend([
            os.path.join(os.path.dirname(__file__), model_path),
            os.path.join(os.path.dirname(__file__), '..', model_path),
        ])

        checkpoint = None
        for p in paths_to_try:
            if not p:
                continue
            tried.append(p)
            try:
                if os.path.exists(p):
                    checkpoint = torch.load(p, map_location=self.device)
                    model_path = p
                    break
            except Exception:
                continue

        if checkpoint is None:
            # æœ€åå°è¯•ç›´æ¥ loadï¼ˆè®© torch æŠ›å‡ºæ›´è¯¦ç»†å¼‚å¸¸ï¼‰
            checkpoint = torch.load(model_path, map_location=self.device)

        self.embeddings = checkpoint['embeddings']
        self.id_maps = checkpoint['id_maps']
        self.config = checkpoint.get('config', {})

        # é¢„å¡«å…… numpy ç¼“å­˜ï¼ˆå»¶è¿Ÿè½¬æ¢ï¼šåªç¼“å­˜å­˜åœ¨çš„ç±»å‹ï¼‰
        for ntype in self.embeddings.keys():
            try:
                arr = self.embeddings[ntype]
                # æ”¯æŒ torch.Tensor æˆ– numpy.ndarray
                if hasattr(arr, 'numpy'):
                    self._emb_np_cache[ntype] = arr.cpu().numpy()
                elif isinstance(arr, np.ndarray):
                    self._emb_np_cache[ntype] = arr
                else:
                    # å°è¯•è½¬æ¢ä¸º ndarray
                    self._emb_np_cache[ntype] = np.array(arr)
            except Exception:
                # å¿½ç•¥ä¸æ”¯æŒçš„ç±»å‹ï¼ŒæŒ‰éœ€ç”Ÿæˆ
                pass

        print(f"âœ… Loaded embeddings from: {model_path}; types: {list(self.embeddings.keys())}")

    def _get_numpy_emb(self, node_type: str):
        """è¿”å›æŒ‡å®š node_type çš„ numpy åµŒå…¥æ•°ç»„ï¼ˆç¼“å­˜ï¼‰"""
        if node_type in self._emb_np_cache:
            return self._emb_np_cache[node_type]

        if node_type in self.embeddings:
            arr = self.embeddings[node_type]
            if hasattr(arr, 'cpu') and hasattr(arr, 'numpy'):
                try:
                    np_arr = arr.cpu().numpy()
                except Exception:
                    np_arr = np.array(arr)
            else:
                np_arr = np.array(arr)

            self._emb_np_cache[node_type] = np_arr
            return np_arr

        raise KeyError(f"Embedding for node type '{node_type}' not found")

    def _safe_neo4j_query(self, query):
        """å®‰å…¨çš„Neo4jæŸ¥è¯¢æ‰§è¡Œ"""
        if self.graph_db is None:
            return []
        try:
            return self.graph_db.run(query).data()
        except Exception as e:
            print(f"âš ï¸ Neo4j query failed: {e}")
            return []
    
    def get_paper_metadata(self, paper_ids: List[str]) -> Dict[str, Any]:
        """æ”¹è¿›çš„å…ƒæ•°æ®è·å– - å¤„ç†å„ç§IDæ ¼å¼"""
        if not paper_ids or self.graph_db is None:
            return self._get_enhanced_fallback_metadata(paper_ids)

        try:
            # ä½¿ç”¨å­—ç¬¦ä¸² ID æŸ¥è¯¢ï¼ˆæ›´é²æ£’ï¼Œæ”¯æŒå­—ç¬¦ä¸²æˆ–æ•°å­—å½¢å¼çš„ IDï¼‰
            batch_size = 50
            all_results = []

            for i in range(0, len(paper_ids), batch_size):
                batch = paper_ids[i:i + batch_size]
                # æ ¼å¼åŒ–ä¸ºå¸¦å¼•å·çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç¡®ä¿ cypher å¯è¯†åˆ«å­—ç¬¦ä¸² id
                formatted_ids = [f"'{str(pid)}'" for pid in batch]
                ids_str = ', '.join(formatted_ids)

                query = f"""
                MATCH (p:Paper)
                WHERE p.paper_id IN [{ids_str}]
                RETURN p.paper_id as paper_id, p.title as title,
                    p.abstract as abstract, p.year as year,
                    p.venue as venue, p.n_citation as citations
                """

                batch_results = self._safe_neo4j_query(query)
                all_results.extend(batch_results)

            # åˆ›å»ºå…ƒæ•°æ®æ˜ å°„
            metadata_map = {}
            for result in all_results:
                paper_id = result['paper_id']
                metadata_map[str(paper_id)] = {
                    'paper_id': str(paper_id),
                    'title': result.get('title', f'Paper {paper_id}'),
                    'abstract': result.get('abstract', 'Abstract not available'),
                    'year': result.get('year', 'Unknown'),
                    'venue': result.get('venue', 'Unknown'),
                    'citation_count': result.get('citations', 0)
                }

            # ä¸ºç¼ºå¤±çš„è®ºæ–‡æ·»åŠ å¤‡ç”¨å…ƒæ•°æ®
            for pid in paper_ids:
                if str(pid) not in metadata_map:
                    metadata_map[str(pid)] = self._create_fallback_metadata(pid)

            return metadata_map

        except Exception as e:
            print(f"âŒ Failed to get paper metadata: {e}")
            return self._get_enhanced_fallback_metadata(paper_ids)

    def _create_fallback_metadata(self, pid):
        """åˆ›å»ºå¤‡ç”¨å…ƒæ•°æ®"""
        pid_str = str(pid)
        return {
            'paper_id': pid_str,
            'title': f"Research Paper {pid_str}",
            'abstract': "Abstract not available in current database.",
            'year': "2023", 
            'venue': "Academic Conference",
            'citation_count': 0,
            'is_fallback': True
        }

    def _get_enhanced_fallback_metadata(self, paper_ids: List[str]) -> Dict[str, Any]:
        """å¢å¼ºçš„å¤‡ç”¨å…ƒæ•°æ®"""
        metadata = {}
        for pid in paper_ids:
            if pid is not None:
                pid_str = str(pid)
                metadata[pid_str] = self._create_fallback_metadata(pid)
        return metadata
    
    def get_author_metadata(self, author_ids: List[str]) -> Dict[str, Any]:
        """ä»Neo4jè·å–ä½œè€…å…ƒæ•°æ®"""
        if not author_ids or self.graph_db is None:
            return {}
        
        try:
            # å¤„ç†IDæ ¼å¼
            formatted_ids = [f"'{aid}'" for aid in author_ids if aid]
            if not formatted_ids:
                return {}
                
            ids_str = ', '.join(formatted_ids)
            
            query = f"""
            MATCH (a:Author)
            WHERE a.author_id IN [{ids_str}]
            RETURN a.author_id as author_id, a.name as name
            """
            results = self._safe_neo4j_query(query)
            return {result['author_id']: dict(result) for result in results}
        except Exception as e:
            print(f"âš ï¸ Failed to get author metadata: {e}")
            return {}
    
    def _diversify_recommendations(self, recommendations: List[Dict], top_k: int) -> List[Dict]:
        """å¤šæ ·æ€§é‡æ’åºç­–ç•¥"""
        if len(recommendations) <= top_k:
            return recommendations
        
        # æŒ‰venueåˆ†ç»„ç¡®ä¿ä¸»é¢˜å¤šæ ·æ€§
        venue_groups = defaultdict(list)
        for rec in recommendations:
            venue = rec.get('venue', 'Unknown')
            venue_groups[venue].append(rec)
        
        diversified = []
        max_per_venue = max(1, top_k // len(venue_groups))
        
        # ä»æ¯ä¸ªä¸»é¢˜ç»„ä¸­é€‰æ‹©ä»£è¡¨æ€§è®ºæ–‡
        for venue, group in venue_groups.items():
            # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©å‰å‡ ä¸ª
            group_sorted = sorted(group, key=lambda x: x.get('similarity_score', 0), reverse=True)
            diversified.extend(group_sorted[:max_per_venue])
        
        # å¦‚æœå¤šæ ·æ€§ç­–ç•¥å¯¼è‡´ç»“æœä¸è¶³ï¼Œç”¨æœ€é«˜åˆ†è¡¥è¶³
        if len(diversified) < top_k:
            remaining = [r for r in recommendations if r not in diversified]
            remaining_sorted = sorted(remaining, key=lambda x: x.get('similarity_score', 0), reverse=True)
            diversified.extend(remaining_sorted[:top_k - len(diversified)])
        
        # é‡æ–°åˆ†é…æ’å
        for i, rec in enumerate(diversified[:top_k]):
            rec['rank'] = i + 1
            rec['diversity_boost'] = True  # æ ‡è®°ç»è¿‡å¤šæ ·æ€§ä¼˜åŒ–
        
        return diversified[:top_k]
    
    def content_based_paper_recommendation(self, query_text: str, top_k: int = 10) -> List[Dict]:
        """åŸºäºå†…å®¹çš„è®ºæ–‡æ¨è - æ”¹è¿›æŠ•å½±æ–¹æ³•"""
        print(f"ğŸ“š Content-based paper recommendation for: {query_text}")
        
        if self.sentence_model is None:
            print("âŒ Sentence transformer not available")
            return []
        
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        try:
            query_embedding = self.sentence_model.encode([query_text])
            print(f"   Query embedding shape: {query_embedding.shape}")
        except Exception as e:
            print(f"âŒ Failed to encode query: {e}")
            return []
        
        # è·å–è®ºæ–‡åµŒå…¥
        if 'paper' not in self.embeddings:
            print("âŒ Paper embeddings not found")
            return []
        paper_embeddings = self._get_numpy_emb('paper')
        print(f"   Paper embeddings shape: {paper_embeddings.shape}")

        # æ”¹è¿›çš„æŠ•å½±æ–¹æ³•
        if query_embedding.shape[1] != paper_embeddings.shape[1]:
            print(f"âš ï¸ Dimension mismatch: query {query_embedding.shape[1]}D vs paper {paper_embeddings.shape[1]}D")
            print("   Using improved projection...")

            # æ–¹æ³•1: ä½¿ç”¨PCAè¿›è¡Œæ›´å¥½çš„æŠ•å½±
            from sklearn.decomposition import PCA

            try:
                # å¦‚æœæŸ¥è¯¢ç»´åº¦æ›´é«˜ï¼Œä½¿ç”¨PCAé™ç»´
                if query_embedding.shape[1] > paper_embeddings.shape[1]:
                    pca = PCA(n_components=paper_embeddings.shape[1])
                    # ä½¿ç”¨è®ºæ–‡åµŒå…¥æ¥æ‹ŸåˆPCAï¼ˆæ¨¡æ‹Ÿåœ¨ç›¸åŒç©ºé—´ï¼‰
                    pca.fit(paper_embeddings)
                    query_projected = pca.transform(query_embedding)
                    paper_projected = paper_embeddings
                else:
                    # å¦‚æœè®ºæ–‡ç»´åº¦æ›´é«˜ï¼Œæå‡æŸ¥è¯¢ç»´åº¦
                    query_projected = np.pad(query_embedding,
                                        ((0,0), (0, paper_embeddings.shape[1] - query_embedding.shape[1])),
                                        mode='constant')
                    paper_projected = paper_embeddings

                # è®¡ç®—ç›¸ä¼¼åº¦
                query_norm = query_projected / np.linalg.norm(query_projected, axis=1, keepdims=True)
                paper_norm = paper_projected / np.linalg.norm(paper_projected, axis=1, keepdims=True)
                similarities = np.dot(query_norm, paper_norm.T)[0]

            except Exception as e:
                print(f"âš ï¸ PCA projection failed, using simple truncation: {e}")
                # å›é€€åˆ°ç®€å•çš„æˆªæ–­æ–¹æ³•
                min_dim = min(query_embedding.shape[1], paper_embeddings.shape[1])
                query_projected = query_embedding[:, :min_dim]
                paper_projected = paper_embeddings[:, :min_dim]

                query_norm = query_projected / np.linalg.norm(query_projected, axis=1, keepdims=True)
                paper_norm = paper_projected / np.linalg.norm(paper_projected, axis=1, keepdims=True)
                similarities = np.dot(query_norm, paper_norm.T)[0]
        else:
            # ç»´åº¦åŒ¹é…ï¼Œæ­£å¸¸è®¡ç®—ï¼›ä¼˜å…ˆä½¿ç”¨çŸ¢é‡åŒ–ç‚¹ç§¯
            try:
                q = np.array(query_embedding)
                p = paper_embeddings
                qn = q / np.linalg.norm(q, axis=1, keepdims=True)
                pn = p / np.linalg.norm(p, axis=1, keepdims=True)
                similarities = (qn @ pn.T)[0]
            except Exception:
                similarities = cosine_similarity(query_embedding, paper_embeddings)[0]
        
        # è·å–top-Kæ¨è
        paper_ids = [self.reverse_maps['paper'].get(i) for i in range(len(paper_embeddings))]
        valid_indices = [i for i, pid in enumerate(paper_ids) if pid is not None and similarities[i] > 0]
        
        if not valid_indices:
            print("âŒ No valid recommendations found")
            return []
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        top_indices = sorted(valid_indices, key=lambda i: similarities[i], reverse=True)[:top_k]
        
        recommendations = []
        for rank, idx in enumerate(top_indices, 1):
            paper_id = paper_ids[idx]
            recommendations.append({
                'paper_id': paper_id,
                'similarity_score': float(similarities[idx]),
                'rank': rank
            })
        
        # æ”¹è¿›å…ƒæ•°æ®è·å–
        print("   Fetching paper metadata...")
        paper_metadata = self.get_paper_metadata([rec['paper_id'] for rec in recommendations])
        
        for rec in recommendations:
            metadata = paper_metadata.get(rec['paper_id'], {})
            rec.update(metadata)
            if 'title' not in rec or not rec['title']:
                rec['title'] = f"Paper {rec['paper_id']}"
        
        print(f"   âœ… Generated {len(recommendations)} recommendations with metadata")
        for rec in recommendations[:3]:
            print(f"      {rec['rank']}. {rec['title']} (score: {rec['similarity_score']:.3f})")
        
        return recommendations
    
    def collaborative_paper_recommendation(self, target_paper_id: str, top_k: int = 10) -> List[Dict]:
        """åŸºäºååŒè¿‡æ»¤çš„è®ºæ–‡æ¨èï¼ˆä½¿ç”¨å›¾ç»“æ„ï¼‰- ä¿®å¤ç‰ˆæœ¬"""
        print(f"ğŸ”— Collaborative paper recommendation for: {target_paper_id}")
        
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print(f"   Checking if target paper exists in embeddings...")
        print(f"   Available paper IDs in id_maps: {len(self.id_maps['paper'])}")
        
        if 'paper' not in self.id_maps:
            print("âŒ Paper id_maps not found")
            return []
        
        if target_paper_id not in self.id_maps['paper']:
            print(f"âš ï¸ Target paper {target_paper_id} not found in embeddings")
            print(f"   Sample paper IDs: {list(self.id_maps['paper'].keys())[:5]}")
            return []
        
        target_idx = self.id_maps['paper'][target_paper_id]
        paper_embeddings = self._get_numpy_emb('paper')

        print(f"   Target paper index: {target_idx}")
        print(f"   Paper embeddings shape: {paper_embeddings.shape}")

        # è®¡ç®—ä¸ç›®æ ‡è®ºæ–‡çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å‘é‡åŒ–ç‚¹ç§¯ä»¥æé«˜é€Ÿåº¦ï¼‰
        try:
            print(f"   Calculating cosine similarities...")
            t = paper_embeddings[target_idx:target_idx+1]
            if np.isnan(t).any() or np.isinf(t).any():
                print("âŒ Target embedding contains NaN or Inf values")
                return []

            t_norm = t / np.linalg.norm(t, axis=1, keepdims=True)
            emb_norm = paper_embeddings / np.linalg.norm(paper_embeddings, axis=1, keepdims=True)
            similarities = (t_norm @ emb_norm.T)[0]

            print(f"   Similarities range: {similarities.min():.3f} to {similarities.max():.3f}")
            print(f"   Number of positive similarities: {np.sum(similarities > 0)}")

        except Exception as e:
            print(f"âŒ Failed to calculate similarities: {e}")
            return []
        
        # æ’é™¤ç›®æ ‡è®ºæ–‡æœ¬èº«
        similarities[target_idx] = -1
        
        # è·å–top-Kæ¨è - æ”¹è¿›é€‰æ‹©é€»è¾‘
        paper_ids = [self.reverse_maps['paper'].get(i) for i in range(len(paper_embeddings))]
        
        # åªé€‰æ‹©æœ‰æ•ˆçš„è®ºæ–‡IDå’Œæ­£ç›¸ä¼¼åº¦
        valid_indices = []
        for i in range(len(paper_ids)):
            if (paper_ids[i] is not None and 
                similarities[i] > 0.1 and  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé¿å…æ¨èä¸ç›¸å…³çš„è®ºæ–‡
                i != target_idx):
                valid_indices.append(i)
        
        print(f"   Valid candidate papers: {len(valid_indices)}")
        
        if not valid_indices:
            print("âš ï¸ No valid recommendations found (all similarities <= 0.1)")
            return []
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é€‰æ‹©top-K
        valid_indices_sorted = sorted(valid_indices, key=lambda i: similarities[i], reverse=True)[:top_k]
        
        recommendations = []
        for rank, idx in enumerate(valid_indices_sorted, 1):
            paper_id = paper_ids[idx]
            recommendations.append({
                'paper_id': paper_id,
                'similarity_score': float(similarities[idx]),
                'rank': rank
            })
        
        print(f"   Generated {len(recommendations)} collaborative recommendations")
        
        # è·å–å…ƒæ•°æ®
        if recommendations:
            print(f"   Fetching metadata for {len(recommendations)} recommendations...")
            recommendation_ids = [rec['paper_id'] for rec in recommendations]
            paper_metadata = self.get_paper_metadata(recommendation_ids)
            print(f"   Retrieved metadata for {len(paper_metadata)} papers")
        
        for rec in recommendations:
            # ä½¿ç”¨å­—ç¬¦ä¸²IDæŸ¥æ‰¾å…ƒæ•°æ®
            paper_id_str = str(rec['paper_id'])
            metadata = paper_metadata.get(paper_id_str, {})
            
            # å¦‚æœä»Neo4jè·å–åˆ°å…ƒæ•°æ®ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨å¤‡ç”¨å…ƒæ•°æ®
            if metadata and metadata.get('title'):
                rec.update(metadata)
            else:
                # ä½¿ç”¨å¤‡ç”¨æ ‡é¢˜
                rec['title'] = f"Research Paper {paper_id_str}"
                rec['year'] = "Unknown"
                rec['venue'] = "Unknown"
        
        # æ˜¾ç¤ºæœ€ç»ˆæ¨èç»“æœ
        print(f"   Final recommendations with metadata:")
        for rec in recommendations[:3]:
            title = rec.get('title', 'Unknown')
            score = rec['similarity_score']
            print(f"      {rec['rank']}. {title} (score: {score:.3f})")
        
        return recommendations
    
    def author_based_recommendation(self, author_id: str, top_k: int = 10) -> List[Dict]:
        """åŸºäºä½œè€…ç›¸ä¼¼æ€§çš„è®ºæ–‡æ¨è"""
        print(f"ğŸ‘¤ Author-based recommendation for author: {author_id}")
        
        if 'author' not in self.id_maps or author_id not in self.id_maps['author']:
            print(f"âš ï¸ Author {author_id} not found in embeddings")
            return []
        
        # è·å–ä½œè€…åµŒå…¥
        author_embeddings = self._get_numpy_emb('author')
        target_idx = self.id_maps['author'][author_id]

        # æ‰¾åˆ°ç›¸ä¼¼ä½œè€…ï¼ˆå‘é‡åŒ–ï¼‰
        try:
            a = author_embeddings[target_idx:target_idx+1]
            a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)
            emb_norm = author_embeddings / np.linalg.norm(author_embeddings, axis=1, keepdims=True)
            author_similarities = (a_norm @ emb_norm.T)[0]
        except Exception as e:
            print(f"âŒ Failed to calculate author similarities: {e}")
            return []
        
        # æ’é™¤ç›®æ ‡ä½œè€…
        author_similarities[target_idx] = -1
        
        # è·å–ç›¸ä¼¼ä½œè€…çš„è®ºæ–‡
        similar_author_indices = np.argsort(author_similarities)[-5:][::-1]  # Top 5ç›¸ä¼¼ä½œè€…
        similar_author_ids = [self.reverse_maps['author'].get(i) for i in similar_author_indices]
        similar_author_ids = [aid for aid in similar_author_ids if aid is not None]
        
        if not similar_author_ids or self.graph_db is None:
            return []
        
        # ä»Neo4jè·å–è¿™äº›ä½œè€…çš„è®ºæ–‡
        try:
            formatted_ids = [f"'{aid}'" for aid in similar_author_ids]
            ids_str = ', '.join(formatted_ids)
            
            query = f"""
            MATCH (a:Author)-[:WRITTEN_BY]-(p:Paper)
            WHERE a.author_id IN [{ids_str}]
            RETURN a.author_id as author_id, p.paper_id as paper_id,
                   p.title as title, p.year as year, p.venue as venue
            ORDER BY p.year DESC
            LIMIT {top_k * 2}
            """
            results = self._safe_neo4j_query(query)
        except Exception as e:
            print(f"âŒ Failed to query author papers: {e}")
            return []
        
        recommendations = []
        seen_papers = set()
        
        for result in results:
            paper_id = result['paper_id']
            if paper_id not in seen_papers:
                author_id_used = result['author_id']
                author_idx = self.id_maps['author'].get(author_id_used)
                author_sim = author_similarities[author_idx] if author_idx is not None else 0
                
                recommendations.append({
                    'paper_id': paper_id,
                    'title': result.get('title', 'Unknown'),
                    'year': result.get('year', 'Unknown'),
                    'venue': result.get('venue', 'Unknown'),
                    'recommended_by_author': author_id_used,
                    'similar_author_score': float(author_sim),
                    'rank': len(recommendations) + 1
                })
                seen_papers.add(paper_id)
            
            if len(recommendations) >= top_k:
                break
        
        return recommendations
    
    def hybrid_paper_recommendation(self, query: str = None, target_paper_id: str = None, 
                                  author_id: str = None, top_k: int = 10) -> Dict[str, Any]:
        """æ··åˆæ¨èï¼šç»“åˆå†…å®¹ã€ååŒè¿‡æ»¤å’Œä½œè€…ä¿¡æ¯"""
        print("ğŸ”„ Generating hybrid recommendations...")
        
        all_recommendations = []
        
        # 1. åŸºäºå†…å®¹çš„æ¨è
        if query:
            content_recs = self.content_based_paper_recommendation(query, top_k*2)
            for rec in content_recs:
                rec['method'] = 'content_based'
                rec['final_score'] = rec['similarity_score'] * 0.4
            all_recommendations.extend(content_recs)
        
        # 2. åŸºäºååŒè¿‡æ»¤çš„æ¨è
        if target_paper_id:
            collab_recs = self.collaborative_paper_recommendation(target_paper_id, top_k*2)
            for rec in collab_recs:
                rec['method'] = 'collaborative'
                rec['final_score'] = rec['similarity_score'] * 0.4
            all_recommendations.extend(collab_recs)
        
        # 3. åŸºäºä½œè€…çš„æ¨è
        if author_id:
            author_recs = self.author_based_recommendation(author_id, top_k*2)
            for rec in author_recs:
                rec['method'] = 'author_based'
                rec['final_score'] = rec['similar_author_score'] * 0.2
            all_recommendations.extend(author_recs)
        
        # å¦‚æœæ²¡æœ‰æ¨èæ–¹æ³•ï¼Œä½¿ç”¨åŸºäºå†…å®¹çš„é»˜è®¤æ¨è
        if not all_recommendations and query:
            content_recs = self.content_based_paper_recommendation(query, top_k)
            for rec in content_recs:
                rec['method'] = 'content_based'
                rec['final_score'] = rec['similarity_score']
            all_recommendations.extend(content_recs)
        
        # åˆå¹¶å’Œé‡æ’åº
        paper_scores = defaultdict(float)
        paper_details = {}
        
        for rec in all_recommendations:
            paper_id = rec['paper_id']
            paper_scores[paper_id] += rec['final_score']
            if paper_id not in paper_details:
                paper_details[paper_id] = rec
        
        # è·å–top-Kæ¨è
        top_papers = heapq.nlargest(top_k, paper_scores.items(), key=lambda x: x[1])
        
        final_recommendations = []
        for paper_id, score in top_papers:
            rec = paper_details[paper_id].copy()
            rec['final_score'] = score
            rec['rank'] = len(final_recommendations) + 1
            final_recommendations.append(rec)
        
        return {
            'recommendations': final_recommendations,
            'query': query,
            'target_paper': target_paper_id,
            'target_author': author_id,
            'total_recommendations': len(final_recommendations)
        }

    def enhanced_collaborative_recommendation(self, target_paper_id: str, top_k: int = 10) -> List[Dict]:
        """å¢å¼ºçš„ååŒè¿‡æ»¤æ¨è - è§£å†³æ•°æ®ç¨€ç–æ€§é—®é¢˜"""
        print(f"ğŸ”— Enhanced collaborative recommendation for: {target_paper_id}")
        
        if 'paper' not in self.id_maps or target_paper_id not in self.id_maps['paper']:
            print(f"âš ï¸ Target paper {target_paper_id} not found")
            return []
        
        target_idx = self.id_maps['paper'][target_paper_id]
        paper_embeddings = self._get_numpy_emb('paper')

        # æ–¹æ³•1: åŸºäºåµŒå…¥çš„ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–ç‚¹ç§¯ï¼‰
        try:
            t = paper_embeddings[target_idx:target_idx+1]
            t_norm = t / np.linalg.norm(t, axis=1, keepdims=True)
            emb_norm = paper_embeddings / np.linalg.norm(paper_embeddings, axis=1, keepdims=True)
            similarities = (t_norm @ emb_norm.T)[0]
        except Exception as e:
            print(f"âŒ Embedding similarity failed: {e}")
            return []
        
        # æ–¹æ³•2: åŸºäºå›¾ç»“æ„çš„ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        graph_similarities = self._calculate_graph_similarity(target_paper_id)
        
        # èåˆä¸¤ç§ç›¸ä¼¼åº¦
        final_similarities = similarities.copy()
        if graph_similarities:
            for paper_id, graph_sim in graph_similarities.items():
                if paper_id in self.id_maps['paper']:
                    idx = self.id_maps['paper'][paper_id]
                    # åŠ æƒèåˆï¼šåµŒå…¥ç›¸ä¼¼åº¦70%ï¼Œå›¾ç›¸ä¼¼åº¦30%
                    final_similarities[idx] = 0.7 * similarities[idx] + 0.3 * graph_sim
        
        # æ’é™¤ç›®æ ‡è®ºæ–‡
        final_similarities[target_idx] = -1
        
        # è·å–æ¨è
        paper_ids = [self.reverse_maps['paper'].get(i) for i in range(len(paper_embeddings))]
        
        valid_indices = []
        for i, pid in enumerate(paper_ids):
            if (pid is not None and 
                pid != target_paper_id and 
                final_similarities[i] > 0.05):  # æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼
                valid_indices.append(i)
        
        if not valid_indices:
            print("âš ï¸ No valid recommendations found")
            return []
        
        # æ’åºå¹¶é€‰æ‹©top-K
        top_indices = sorted(valid_indices, key=lambda i: final_similarities[i], reverse=True)[:top_k]
        
        recommendations = []
        for rank, idx in enumerate(top_indices, 1):
            paper_id = paper_ids[idx]
            recommendations.append({
                'paper_id': paper_id,
                'similarity_score': float(final_similarities[idx]),
                'embedding_similarity': float(similarities[idx]),
                'graph_similarity': graph_similarities.get(paper_id, 0.0),
                'rank': rank
            })
        
        # è·å–å…ƒæ•°æ®
        paper_metadata = self.get_paper_metadata([rec['paper_id'] for rec in recommendations])
        for rec in recommendations:
            metadata = paper_metadata.get(str(rec['paper_id']), {})
            rec.update(metadata)
        
        print(f"âœ… Enhanced collaborative: {len(recommendations)} recommendations")
        return recommendations

    def _calculate_graph_similarity(self, target_paper_id: str) -> Dict[str, float]:
        """åŸºäºå›¾ç»“æ„è®¡ç®—è®ºæ–‡ç›¸ä¼¼åº¦"""
        if self.graph_db is None:
            return {}
        
        try:
            # æŸ¥è¯¢å…±åŒå¼•ç”¨ã€å…±åŒä½œè€…ç­‰å›¾å…³ç³»
            query = """
            MATCH (p1:Paper {paper_id: $target_id})
            OPTIONAL MATCH (p1)-[:CITES]-(common:Paper)-[:CITES]-(p2:Paper)
            WHERE p2.paper_id <> $target_id
            WITH p2, COUNT(common) as common_citations
            OPTIONAL MATCH (p1)-[:WRITTEN_BY]-(a:Author)-[:WRITTEN_BY]-(p2)
            WITH p2, common_citations, COUNT(a) as common_authors
            RETURN p2.paper_id as paper_id, 
                (common_citations * 0.6 + common_authors * 0.4) as graph_similarity
            ORDER BY graph_similarity DESC
            LIMIT 20
            """
            
            results = self.graph_db.run(query, target_id=str(target_paper_id)).data()
            return {str(result['paper_id']): float(result['graph_similarity']) for result in results}
            
        except Exception as e:
            print(f"âš ï¸ Graph similarity calculation failed: {e}")
            return {}
        
    def optimized_hybrid_recommendation(self, query: str = None, target_paper_id: str = None, 
                                    author_id: str = None, top_k: int = 10) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ··åˆæ¨è - åŠ¨æ€æƒé‡è°ƒæ•´"""
        print("ğŸ”„ Running optimized hybrid recommendation...")
        
        all_recommendations = []
        method_weights = self._calculate_adaptive_weights(query, target_paper_id, author_id)
        
        print(f"   Dynamic weights: {method_weights}")
        
        # 1. åŸºäºå†…å®¹çš„æ¨è
        if query and method_weights['content'] > 0:
            content_recs = self.content_based_paper_recommendation(query, top_k*3)
            for rec in content_recs:
                rec['method'] = 'content_based'
                rec['final_score'] = rec['similarity_score'] * method_weights['content']
                rec['method_weight'] = method_weights['content']
            all_recommendations.extend(content_recs)
        
        # 2. å¢å¼ºçš„ååŒè¿‡æ»¤æ¨è
        if target_paper_id and method_weights['collaborative'] > 0:
            collab_recs = self.enhanced_collaborative_recommendation(target_paper_id, top_k*3)
            for rec in collab_recs:
                rec['method'] = 'collaborative_enhanced'
                rec['final_score'] = rec['similarity_score'] * method_weights['collaborative']
                rec['method_weight'] = method_weights['collaborative']
            all_recommendations.extend(collab_recs)
        
        # 3. åŸºäºä½œè€…çš„æ¨è
        if author_id and method_weights['author'] > 0:
            author_recs = self.author_based_recommendation(author_id, top_k*2)
            for rec in author_recs:
                rec['method'] = 'author_based'
                rec['final_score'] = rec['similar_author_score'] * method_weights['author']
                rec['method_weight'] = method_weights['author']
            all_recommendations.extend(author_recs)
        
        # å¤šæ ·æ€§é‡æ’åº
        final_recommendations = self._diversified_reranking(all_recommendations, top_k)
        
        return {
            'recommendations': final_recommendations,
            'query': query,
            'target_paper': target_paper_id,
            'target_author': author_id,
            'method_weights': method_weights,
            'total_recommendations': len(final_recommendations)
        }

    def _calculate_adaptive_weights(self, query: str = None, target_paper_id: str = None, 
                              author_id: str = None) -> Dict[str, float]:
        """è‡ªé€‚åº”æƒé‡è®¡ç®—"""
        weights = {'content': 0.0, 'collaborative': 0.0, 'author': 0.0}
        
        # åŸºäºè¾“å…¥è´¨é‡è°ƒæ•´æƒé‡
        if query and len(query.strip()) > 10:  # æŸ¥è¯¢è¾ƒé•¿ï¼Œå†…å®¹æƒé‡æ›´é«˜
            weights['content'] += 0.5
        elif query:
            weights['content'] += 0.3
        
        if target_paper_id:
            # æ£€æŸ¥ç›®æ ‡è®ºæ–‡æ˜¯å¦æœ‰è¶³å¤Ÿçš„è¿æ¥
            connection_strength = self._evaluate_paper_connections(target_paper_id)
            weights['collaborative'] += 0.3 + (0.2 * connection_strength)
        
        if author_id:
            # æ£€æŸ¥ä½œè€…çš„æ´»è·ƒåº¦
            author_activity = self._evaluate_author_activity(author_id)
            weights['author'] += 0.2 + (0.1 * author_activity)
        
        # å½’ä¸€åŒ–
        total = sum(weights.values())
        if total > 0:
            weights = {k: v/total for k, v in weights.items()}
        else:
            # é»˜è®¤æƒé‡
            weights = {'content': 0.5, 'collaborative': 0.3, 'author': 0.2}
        
        return weights

    def _evaluate_paper_connections(self, paper_id: str) -> float:
        """è¯„ä¼°è®ºæ–‡çš„è¿æ¥å¼ºåº¦"""
        if self.graph_db is None:
            return 0.5  # é»˜è®¤ä¸­ç­‰å¼ºåº¦
        
        try:
            query = """
            MATCH (p:Paper {paper_id: $paper_id})
            OPTIONAL MATCH (p)-[:CITES]-(cited)
            OPTIONAL MATCH (p)-[:WRITTEN_BY]-(authors)
            WITH COUNT(DISTINCT cited) as citation_count, 
                COUNT(DISTINCT authors) as author_count
            RETURN (citation_count * 0.7 + author_count * 0.3) as connection_strength
            """
            result = self.graph_db.run(query, paper_id=str(paper_id)).data()
            if result and result[0]['connection_strength']:
                return min(1.0, result[0]['connection_strength'] / 10.0)  # å½’ä¸€åŒ–
        except:
            pass
        
        return 0.5

    def _evaluate_author_activity(self, author_id: str) -> float:
        """è¯„ä¼°ä½œè€…æ´»è·ƒåº¦"""
        if self.graph_db is None:
            return 0.5
        
        try:
            query = """
            MATCH (a:Author {author_id: $author_id})-[:WRITTEN_BY]-(p:Paper)
            WITH COUNT(p) as paper_count
            RETURN CASE 
                WHEN paper_count > 10 THEN 1.0
                WHEN paper_count > 5 THEN 0.7
                WHEN paper_count > 2 THEN 0.4
                ELSE 0.2
            END as activity_level
            """
            result = self.graph_db.run(query, author_id=author_id).data()
            if result:
                return result[0]['activity_level']
        except:
            pass
        
        return 0.5

    def _diversified_reranking(self, recommendations, top_k):
        """å¤šæ ·æ€§é‡æ’åº"""
        if not recommendations:
            return []
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_by_score = sorted(recommendations, key=lambda x: x.get('final_score', 0), reverse=True)
        
        # ç¡®ä¿æ–¹æ³•å¤šæ ·æ€§
        final_recs = []
        method_count = {'content_based': 0, 'collaborative_enhanced': 0, 'author_based': 0}
        max_per_method = max(1, top_k // 3)  # æ¯ç§æ–¹æ³•æœ€å¤šå 1/3
        
        for rec in sorted_by_score:
            method = rec.get('method', 'unknown')
            if method_count.get(method, 0) < max_per_method:
                final_recs.append(rec)
                method_count[method] = method_count.get(method, 0) + 1
            
            if len(final_recs) >= top_k:
                break
        
        # å¦‚æœå¤šæ ·æ€§é™åˆ¶å¯¼è‡´ç»“æœä¸è¶³ï¼Œç”¨æœ€é«˜åˆ†è¡¥è¶³
        if len(final_recs) < top_k:
            for rec in sorted_by_score:
                if rec not in final_recs:
                    final_recs.append(rec)
                if len(final_recs) >= top_k:
                    break
        
        # é‡æ–°åˆ†é…æ’å
        for i, rec in enumerate(final_recs):
            rec['rank'] = i + 1
        
        return final_recs

    def _generate_recommendations_from_scores(self, scores, target_idx, top_k, method):
        """ä»åˆ†æ•°ç”Ÿæˆæ¨èç»“æœ"""
        paper_ids = [self.reverse_maps['paper'].get(i) for i in range(len(scores))]
        
        valid_indices = []
        for i, pid in enumerate(paper_ids):
            if (pid is not None and 
                i != target_idx and 
                scores[i] > 0.05):
                valid_indices.append(i)
        
        if not valid_indices:
            return []
        
        top_indices = sorted(valid_indices, key=lambda i: scores[i], reverse=True)[:top_k]
        
        recommendations = []
        for rank, idx in enumerate(top_indices, 1):
            paper_id = paper_ids[idx]
            recommendations.append({
                'paper_id': paper_id,
                'similarity_score': float(scores[idx]),
                'rank': rank,
                'method': method
            })
        
        # è·å–å…ƒæ•°æ®
        paper_metadata = self.get_paper_metadata([rec['paper_id'] for rec in recommendations])
        for rec in recommendations:
            metadata = paper_metadata.get(str(rec['paper_id']), {})
            rec.update(metadata)
        
        return recommendations

class CollaboratorRecommender:
    def __init__(self, 
                 model_path=r"training\models\trial5\han_embeddings.pth",
                 neo4j_uri="neo4j://127.0.0.1:7687",
                 neo4j_username="neo4j", 
                 neo4j_password="12345678"):
        """åˆå§‹åŒ–åˆä½œè€…æ¨èç³»ç»Ÿ"""
        self.device = torch.device('cpu')
        
        # åŠ è½½åµŒå…¥
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            self.embeddings = checkpoint['embeddings']
            self.id_maps = checkpoint['id_maps']
            print(f"âœ… Loaded collaborator embeddings: {[f'{k}: {v.shape}' for k, v in self.embeddings.items() if 'author' in k]}")
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            raise
        
        # è¿æ¥Neo4j
        try:
            self.graph_db = Graph(neo4j_uri, auth=(neo4j_username, neo4j_password))
            self.graph_db.run("RETURN 1")
            print("âœ… Neo4j connection successful")
        except Exception as e:
            print(f"âŒ Failed to connect to Neo4j: {e}")
            self.graph_db = None
        
        # æ„å»ºåå‘æ˜ å°„
        self.reverse_maps = {}
        for node_type, id_map in self.id_maps.items():
            self.reverse_maps[node_type] = {v: k for k, v in id_map.items()}
        
        # æ„å»ºåˆä½œç½‘ç»œ
        self._build_collaboration_network()
    
    def _safe_neo4j_query(self, query):
        """å®‰å…¨çš„Neo4jæŸ¥è¯¢æ‰§è¡Œ"""
        if self.graph_db is None:
            return []
        try:
            return self.graph_db.run(query).data()
        except Exception as e:
            print(f"âš ï¸ Neo4j query failed: {e}")
            return []
    
    def _build_collaboration_network(self):
        """æ„å»ºä½œè€…åˆä½œç½‘ç»œ"""
        print("ğŸ”¨ Building collaboration network...")
        
        import time
        start_time = time.time()
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨æ›´ç®€å•çš„æŸ¥è¯¢ï¼Œé™åˆ¶æ•°æ®é‡
            query = """
            MATCH (a1:Author)-[:WRITTEN_BY]-(p:Paper)-[:WRITTEN_BY]-(a2:Author)
            WHERE a1.author_id <> a2.author_id
            WITH a1, a2, COUNT(p) as collaboration_count
            WHERE collaboration_count >= 1  // è‡³å°‘åˆä½œè¿‡ä¸€æ¬¡
            RETURN a1.author_id as author1, a2.author_id as author2, collaboration_count
            LIMIT 5000  // é™åˆ¶æ•°é‡é¿å…è¶…æ—¶
            """
            
            print("   Executing optimized Neo4j query...")
            results = self._safe_neo4j_query(query)
            
            if not results:
                print("âš ï¸ No collaboration data found in Neo4j, creating empty network")
                self.collab_network = nx.Graph()
                return
            
            print(f"   Retrieved {len(results)} collaboration records")
            
            # åˆ›å»ºåˆä½œç½‘ç»œ
            self.collab_network = nx.Graph()
            
            # å¿«é€Ÿæ„å»ºç½‘ç»œï¼Œä¸æ˜¾ç¤ºè¿›åº¦æ¡
            for result in results:
                author1 = result['author1']
                author2 = result['author2']
                count = result['collaboration_count']
                
                self.collab_network.add_edge(author1, author2, weight=count)
            
            elapsed_time = time.time() - start_time
            print(f"âœ… Collaboration network built in {elapsed_time:.2f}s: {self.collab_network.number_of_nodes()} authors, "
                f"{self.collab_network.number_of_edges()} collaborations")
            
        except Exception as e:
            print(f"âŒ Failed to build collaboration network: {e}")
            print("âš ï¸ Creating empty collaboration network as fallback")
            # åˆ›å»ºç©ºç½‘ç»œä½œä¸ºå›é€€
            self.collab_network = nx.Graph()
    
    def embedding_based_collaborator_recommendation(self, author_id: str, top_k: int = 10) -> List[Dict]:
        """åŸºäºåµŒå…¥ç›¸ä¼¼æ€§çš„åˆä½œè€…æ¨è"""
        if 'author' not in self.id_maps or author_id not in self.id_maps['author']:
            print(f"âš ï¸ Author {author_id} not found in embeddings")
            return []
        
        # å…¼å®¹ torch tensor æˆ– numpy array
        arr = self.embeddings['author']
        if hasattr(arr, 'cpu') and hasattr(arr, 'numpy'):
            author_embeddings = arr.cpu().numpy()
        elif isinstance(arr, np.ndarray):
            author_embeddings = arr
        else:
            author_embeddings = np.array(arr)
        target_idx = self.id_maps['author'][author_id]
        
        # è®¡ç®—ä½œè€…ç›¸ä¼¼åº¦
        try:
            a = author_embeddings[target_idx:target_idx+1]
            a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)
            emb_norm = author_embeddings / np.linalg.norm(author_embeddings, axis=1, keepdims=True)
            similarities = (a_norm @ emb_norm.T)[0]
        except Exception as e:
            print(f"âŒ Failed to calculate author similarities: {e}")
            return []
        
        # æ’é™¤ç›®æ ‡ä½œè€…å’Œå·²æœ‰åˆä½œè€…
        similarities[target_idx] = -1
        
        # è·å–ç°æœ‰åˆä½œè€…
        if author_id in self.collab_network:
            existing_collaborators = list(self.collab_network[author_id].keys())
            for collab_id in existing_collaborators:
                if collab_id in self.id_maps['author']:
                    collab_idx = self.id_maps['author'][collab_id]
                    similarities[collab_idx] = -1
        
        # è·å–top-Kæ¨è
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        author_ids = [self.reverse_maps['author'].get(i) for i in range(len(author_embeddings))]
        author_ids = [aid for aid in author_ids if aid is not None]
        
        recommendations = []
        for idx in top_indices:
            if (idx < len(author_ids) and author_ids[idx] is not None and 
                similarities[idx] > 0):  # åªä¿ç•™æ­£ç›¸ä¼¼åº¦
                rec_author_id = author_ids[idx]
                recommendations.append({
                    'author_id': rec_author_id,
                    'similarity_score': float(similarities[idx]),
                    'rank': len(recommendations) + 1
                })
        
        # è·å–ä½œè€…å…ƒæ•°æ®
        author_metadata = self.get_author_metadata([rec['author_id'] for rec in recommendations])
        for rec in recommendations:
            rec.update(author_metadata.get(rec['author_id'], {}))
        
        return recommendations
    
    def network_based_collaborator_recommendation(self, author_id: str, top_k: int = 10) -> List[Dict]:
        """åŸºäºç½‘ç»œç»“æ„çš„åˆä½œè€…æ¨èï¼ˆå…±åŒåˆä½œè€…ï¼‰"""
        if author_id not in self.collab_network:
            print(f"âš ï¸ Author {author_id} not found in collaboration network")
            return []
        
        # ä½¿ç”¨å…±åŒé‚»å±…ä½œä¸ºæ¨èä¾æ®
        recommendations = []
        seen_authors = set([author_id])
        
        # ç›´æ¥åˆä½œè€…ï¼ˆå·²å­˜åœ¨ï¼‰
        direct_collaborators = list(self.collab_network[author_id].keys())
        seen_authors.update(direct_collaborators)
        
        # æ¨èå…±åŒåˆä½œè€…ï¼ˆæœ‹å‹çš„æœ‹å‹ï¼‰
        for collab in direct_collaborators:
            if collab in self.collab_network:
                for potential_collab in self.collab_network[collab]:
                    if (potential_collab not in seen_authors and 
                        potential_collab != author_id):
                        
                        # è®¡ç®—å…±åŒåˆä½œè€…æ•°é‡
                        common_collabs = []
                        if author_id in self.collab_network and potential_collab in self.collab_network:
                            common_collabs = set(self.collab_network[author_id].keys()) & \
                                           set(self.collab_network[potential_collab].keys())
                        
                        jaccard_similarity = len(common_collabs) / len(
                            set(self.collab_network[author_id].keys()) | 
                            set(self.collab_network[potential_collab].keys())
                        ) if (author_id in self.collab_network and potential_collab in self.collab_network and 
                              len(self.collab_network[author_id]) > 0 and len(self.collab_network[potential_collab]) > 0) else 0
                        
                        recommendations.append({
                            'author_id': potential_collab,
                            'common_collaborators': len(common_collabs),
                            'jaccard_similarity': jaccard_similarity,
                            'recommended_via': collab
                        })
                        seen_authors.add(potential_collab)
        
        # æŒ‰å…±åŒåˆä½œè€…æ•°é‡æ’åº
        recommendations.sort(key=lambda x: (x['common_collaborators'], x['jaccard_similarity']), reverse=True)
        
        # é™åˆ¶æ•°é‡å¹¶æ·»åŠ æ’å
        final_recommendations = []
        for i, rec in enumerate(recommendations[:top_k]):
            rec['rank'] = i + 1
            final_recommendations.append(rec)
        
        # è·å–ä½œè€…å…ƒæ•°æ®
        author_metadata = self.get_author_metadata([rec['author_id'] for rec in final_recommendations])
        for rec in final_recommendations:
            rec.update(author_metadata.get(rec['author_id'], {}))
        
        return final_recommendations
    
    def community_based_recommendation(self, author_id: str, top_k: int = 10) -> List[Dict]:
        """åŸºäºç¤¾åŒºæ£€æµ‹çš„åˆä½œè€…æ¨è"""
        if author_id not in self.collab_network:
            return []
        
        # ä½¿ç”¨Louvainæ–¹æ³•æ£€æµ‹ç¤¾åŒº
        try:
            communities = nx.community.louvain_communities(self.collab_network)
        except:
            # å¦‚æœLouvainå¤±è´¥ï¼Œä½¿ç”¨è¿é€šç»„ä»¶
            communities = list(nx.connected_components(self.collab_network))
        
        # æ‰¾åˆ°ç›®æ ‡ä½œè€…çš„ç¤¾åŒº
        target_community = None
        for community in communities:
            if author_id in community:
                target_community = community
                break
        
        if not target_community:
            return []
        
        recommendations = []
        for candidate in target_community:
            if (candidate != author_id and 
                (author_id not in self.collab_network or candidate not in self.collab_network[author_id])):
                
                # åœ¨ç¤¾åŒºå†…ä½†å°šæœªåˆä½œ
                recommendations.append({
                    'author_id': candidate,
                    'same_community': True,
                    'community_size': len(target_community)
                })
        
        # é™åˆ¶æ•°é‡
        recommendations = recommendations[:top_k]
        
        # è·å–ä½œè€…å…ƒæ•°æ®
        author_metadata = self.get_author_metadata([rec['author_id'] for rec in recommendations])
        for i, rec in enumerate(recommendations):
            rec.update(author_metadata.get(rec['author_id'], {}))
            rec['rank'] = i + 1
        
        return recommendations
    
    def hybrid_collaborator_recommendation(self, author_id: str, top_k: int = 10) -> Dict[str, Any]:
        """æ··åˆåˆä½œè€…æ¨è"""
        print(f"ğŸ‘¥ Hybrid collaborator recommendation for: {author_id}")
        
        all_recommendations = []
        
        # 1. åŸºäºåµŒå…¥çš„æ¨è
        embedding_recs = self.embedding_based_collaborator_recommendation(author_id, top_k*2)
        for rec in embedding_recs:
            rec['method'] = 'embedding_based'
            rec['final_score'] = rec['similarity_score'] * 0.5
            all_recommendations.append(rec)
        
        # 2. åŸºäºç½‘ç»œçš„æ¨è
        network_recs = self.network_based_collaborator_recommendation(author_id, top_k*2)
        for rec in network_recs:
            rec['method'] = 'network_based'
            rec['final_score'] = (rec['common_collaborators'] * 0.05 + 
                                rec['jaccard_similarity'] * 0.3)
            all_recommendations.append(rec)
        
        # 3. åŸºäºç¤¾åŒºçš„æ¨è
        community_recs = self.community_based_recommendation(author_id, top_k)
        for rec in community_recs:
            rec['method'] = 'community_based'
            rec['final_score'] = 0.2  # åŸºç¡€åˆ†æ•°
            all_recommendations.append(rec)
        
        # åˆå¹¶å’Œé‡æ’åº
        author_scores = defaultdict(float)
        author_details = {}
        
        for rec in all_recommendations:
            author_id_rec = rec['author_id']
            author_scores[author_id_rec] += rec['final_score']
            if author_id_rec not in author_details:
                author_details[author_id_rec] = rec
        
        # è·å–top-Kæ¨è
        top_authors = heapq.nlargest(top_k, author_scores.items(), key=lambda x: x[1])
        
        final_recommendations = []
        for author_id_rec, score in top_authors:
            rec = author_details[author_id_rec].copy()
            rec['final_score'] = score
            rec['rank'] = len(final_recommendations) + 1
            final_recommendations.append(rec)
        
        return {
            'recommendations': final_recommendations,
            'target_author': author_id,
            'total_recommendations': len(final_recommendations),
            'collaboration_network_stats': {
                'total_collaborators': len(self.collab_network[author_id]) if author_id in self.collab_network else 0,
                'network_size': self.collab_network.number_of_nodes(),
                'total_collaborations': self.collab_network.number_of_edges()
            }
        }
    
    def get_author_metadata(self, author_ids: List[str]) -> Dict[str, Any]:
        """è·å–ä½œè€…å…ƒæ•°æ®"""
        if not author_ids or self.graph_db is None:
            return {}
        
        try:
            # å¤„ç†IDæ ¼å¼
            formatted_ids = [f"'{aid}'" for aid in author_ids if aid]
            if not formatted_ids:
                return {}
                
            ids_str = ', '.join(formatted_ids)
            
            query = f"""
            MATCH (a:Author)
            WHERE a.author_id IN [{ids_str}]
            RETURN a.author_id as author_id, a.name as name
            """
            results = self._safe_neo4j_query(query)
            return {result['author_id']: dict(result) for result in results}
        except Exception as e:
            print(f"âš ï¸ Failed to get author metadata: {e}")
            return {}

class RecommendationMonitor:
    """æ¨èè´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.performance_history = []
    
    def log_recommendation_quality(self, recommendations: List[Dict], method: str):
        """è®°å½•æ¨èè´¨é‡æŒ‡æ ‡"""
        if not recommendations:
            return
        
        quality_metrics = {
            'method': method,
            'timestamp': datetime.now().isoformat(),
            'count': len(recommendations),
            'avg_score': np.mean([r.get('similarity_score', 0) for r in recommendations]),
            'score_std': np.std([r.get('similarity_score', 0) for r in recommendations]),
            'diversity': self._calculate_diversity(recommendations),
            'venue_coverage': len(set(r.get('venue', 'Unknown') for r in recommendations))
        }
        
        self.performance_history.append(quality_metrics)
        
        # ä¿æŒå†å²è®°å½•å¤§å°
        if len(self.performance_history) > 1000:
            self.performance_history = self.performance_history[-1000:]
    
    def _calculate_diversity(self, recommendations: List[Dict]) -> float:
        """è®¡ç®—æ¨èå¤šæ ·æ€§"""
        if not recommendations:
            return 0.0
        
        venues = [r.get('venue', 'Unknown') for r in recommendations]
        unique_venues = len(set(venues))
        return unique_venues / len(venues)
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.performance_history:
            return {}
        
        df = pd.DataFrame(self.performance_history)
        report = {
            'total_recommendations': len(self.performance_history),
            'methods_used': df['method'].value_counts().to_dict(),
            'average_scores_by_method': df.groupby('method')['avg_score'].mean().to_dict(),
            'average_diversity_by_method': df.groupby('method')['diversity'].mean().to_dict()
        }
        
        return report

def main():
    """æµ‹è¯•æ¨èç³»ç»Ÿ"""
    print("ğŸ¯ Testing Academic Recommendation System")
    
    try:
        # åˆå§‹åŒ–æ¨èå™¨
        paper_recommender = AcademicRecommender()
        collaborator_recommender = CollaboratorRecommender()
        
        # æµ‹è¯•è®ºæ–‡æ¨è
        print("\n" + "="*70)
        print("ğŸ“š Testing Paper Recommendation")
        print("="*70)
        
        # åŸºäºå†…å®¹çš„æ¨è
        content_recs = paper_recommender.content_based_paper_recommendation(
            "graph neural networks for recommender systems", top_k=3
        )
        print(f"Content-based recommendations: {len(content_recs)} papers")
        for rec in content_recs[:2]:
            print(f"  - {rec.get('title', 'Unknown')} (score: {rec['similarity_score']:.3f})")
        
        # æµ‹è¯•åˆä½œè€…æ¨è
        print("\n" + "="*70)
        print("ğŸ‘¥ Testing Collaborator Recommendation")
        print("="*70)
        
        if collaborator_recommender.id_maps['author']:
            sample_author = list(collaborator_recommender.id_maps['author'].keys())[0]
            collab_recs = collaborator_recommender.hybrid_collaborator_recommendation(
                sample_author, top_k=3
            )
            print(f"Collaborator recommendations for author: {len(collab_recs['recommendations'])} authors")
            for rec in collab_recs['recommendations'][:2]:
                print(f"  - {rec.get('name', 'Unknown')} (score: {rec['final_score']:.3f})")
        
        print("\nâœ… All tests completed successfully!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()