#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Test Trial 5 recommender system performance
ç›´æ¥æµ‹è¯•Trial 5æ¨èæ•ˆæœ
"""

import os
import sys
import json

def main():
    print("=" * 70)
    print("ğŸ§ª Testing Trial 5 Recommender System")
    print("=" * 70)
    
    try:
        # å¯¼å…¥æ¨èç³»ç»Ÿ
        from recommender_system import AcademicRecommender
        from evaluate_recommender import RecommenderEvaluator, analyze_data_issues
        
        print("\nğŸ“¥ Initializing recommender system with Trial 5 model...")
        recommender = AcademicRecommender(
            model_path="models/trial5/han_embeddings.pth",
            neo4j_uri="neo4j://127.0.0.1:7687",
            neo4j_username="neo4j",
            neo4j_password="87654321"
        )
        print("âœ… Recommender system initialized successfully!")
        
        # åˆ›å»ºè¯„ä¼°å™¨ - ä½¿ç”¨filteredç­–ç•¥
        print("\nğŸ“Š Creating evaluator with 'filtered' strategy...")
        evaluator = RecommenderEvaluator(
            recommender,
            k_values=[5, 10, 20],
            evaluation_strategy="filtered"
        )
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data_path = "data/processed/test/papers.csv"
        if not os.path.exists(test_data_path):
            print(f"âŒ Test data not found: {test_data_path}")
            print("ğŸ’¡ Available paths:")
            for root, dirs, files in os.walk("data/processed/test"):
                for file in files:
                    print(f"   - {os.path.join(root, file)}")
            return
        
        print(f"\nğŸ“‹ Loading test data from {test_data_path}...")
        evaluator.load_test_data(test_data_path)
        evaluator.prepare_ground_truth()
        
        # åˆ†ææ•°æ®è¦†ç›–æƒ…å†µ
        print("\nğŸ“Š Analyzing data coverage...")
        data_analysis = analyze_data_issues(recommender, test_data_path)
        print(f"   Coverage: {data_analysis['coverage']:.1%}")
        print(f"   Common papers: {data_analysis['common_papers']}")
        print(f"   Missing papers: {data_analysis['missing_papers']}")
        
        # è·å–å¯ç”¨è®ºæ–‡
        available_papers = evaluator._get_available_test_papers()
        print(f"\nğŸ“ Available test papers: {len(available_papers)}")
        
        if not available_papers:
            print("âŒ No test papers available for evaluation")
            return
        
        # è¯„ä¼°ååŒè¿‡æ»¤æ–¹æ³•
        print("\n" + "=" * 70)
        print("ğŸ“Š Evaluating Collaborative Filtering")
        print("=" * 70)
        
        sample_size = min(20, len(available_papers))
        print(f"\nğŸ”„ Evaluating {sample_size} papers...")
        
        collab_results = evaluator.evaluate_batch(
            available_papers[:sample_size], 
            "collaborative"
        )
        
        if "average_metrics" in collab_results:
            print("\nğŸ“ˆ Results Summary:")
            for k in [5, 10, 20]:
                if k in collab_results["average_metrics"]:
                    metrics = collab_results["average_metrics"][k]
                    print(f"\n  K={k}:")
                    print(f"    Precision:  {metrics['precision']['mean']:.4f} Â± {metrics['precision']['std']:.4f}")
                    print(f"    Recall:     {metrics['recall']['mean']:.4f} Â± {metrics['recall']['std']:.4f}")
                    print(f"    F1:         {metrics['f1']['mean']:.4f} Â± {metrics['f1']['std']:.4f}")
                    print(f"    NDCG:       {metrics['ndcg']['mean']:.4f} Â± {metrics['ndcg']['std']:.4f}")
                    print(f"    Hit Rate:   {metrics['hit_rate']['mean']:.4f}")
                    print(f"    Coverage:   {metrics['coverage']['mean']:.1%}")
        
        # å°è¯•è¯„ä¼°å†…å®¹åŸºç¡€æ–¹æ³•
        print("\n" + "=" * 70)
        print("ğŸ“Š Evaluating Content-Based Method")
        print("=" * 70)
        
        print(f"\nğŸ”„ Evaluating {sample_size} papers...")
        
        try:
            content_results = evaluator.evaluate_batch(
                available_papers[:sample_size],
                "content"
            )
            
            if "average_metrics" in content_results:
                print("\nğŸ“ˆ Results Summary:")
                for k in [5, 10, 20]:
                    if k in content_results["average_metrics"]:
                        metrics = content_results["average_metrics"][k]
                        print(f"\n  K={k}:")
                        print(f"    Precision:  {metrics['precision']['mean']:.4f} Â± {metrics['precision']['std']:.4f}")
                        print(f"    Recall:     {metrics['recall']['mean']:.4f} Â± {metrics['recall']['std']:.4f}")
                        print(f"    F1:         {metrics['f1']['mean']:.4f} Â± {metrics['f1']['std']:.4f}")
                        print(f"    NDCG:       {metrics['ndcg']['mean']:.4f} Â± {metrics['ndcg']['std']:.4f}")
        except Exception as e:
            print(f"âš ï¸  Content-based evaluation failed: {e}")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ’¾ Saving Results")
        print("=" * 70)
        
        final_results = {
            "model": "Trial 5",
            "model_path": "models/trial5/han_embeddings.pth",
            "evaluation_strategy": "filtered",
            "data_coverage": data_analysis,
            "collaborative_filtering": collab_results,
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        output_dir = "evaluation_results"
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, "trial5_evaluation.json")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            import numpy as np
            def convert_to_serializable(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, dict):
                    return {k: convert_to_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, (list, tuple)):
                    return [convert_to_serializable(item) for item in obj]
                return obj
            
            json.dump(convert_to_serializable(final_results), f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Results saved to {output_path}")
        
        # æœ€ç»ˆæ€»ç»“
        print("\n" + "=" * 70)
        print("ğŸ“‹ FINAL SUMMARY")
        print("=" * 70)
        print(f"Model: Trial 5")
        print(f"Evaluation Strategy: Filtered (only papers in database)")
        print(f"Test Papers: {sample_size}/{len(available_papers)}")
        print(f"Data Coverage: {data_analysis['coverage']:.1%}")
        
        if "average_metrics" in collab_results:
            k = 10
            if k in collab_results["average_metrics"]:
                metrics = collab_results["average_metrics"][k]
                print(f"\nBest Results (K=10):")
                print(f"  Precision@10:  {metrics['precision']['mean']:.4f}")
                print(f"  Recall@10:     {metrics['recall']['mean']:.4f}")
                print(f"  NDCG@10:       {metrics['ndcg']['mean']:.4f}")
                print(f"  Hit Rate@10:   {metrics['hit_rate']['mean']:.2%}")
        
        print("\nâœ… Evaluation completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Error during evaluation: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    import pandas as pd
    sys.exit(main())
