import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
import dgl.nn as dglnn
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sentence_transformers import SentenceTransformer
from py2neo import Graph
from tqdm import tqdm
import os
import time
from datetime import datetime, timedelta
import json

class HANModel(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, meta_paths):
        super(HANModel, self).__init__()
        
        self.layers = nn.ModuleList()
        self.layers.append(dglnn.HeteroGraphConv({
            rel: dglnn.GATConv(in_dim, hidden_dim, num_heads)
            for rel in meta_paths
        }))
        
        self.layers.append(dglnn.HeteroGraphConv({
            rel: dglnn.GATConv(hidden_dim * num_heads, hidden_dim, num_heads)
            for rel in meta_paths
        }))
        
        # Final projection layer to match output dimension with input (384)
        self.output_projection = nn.Linear(hidden_dim * num_heads, out_dim)
        
        # Batch normalization to stabilize embeddings and prevent collapse
        self.batch_norms = nn.ModuleDict({
            'paper': nn.BatchNorm1d(out_dim, momentum=0.9),
            'author': nn.BatchNorm1d(out_dim, momentum=0.9),
            'keyword': nn.BatchNorm1d(out_dim, momentum=0.9)
        })
        
    def forward(self, graph, inputs, apply_batchnorm=True):
        h = inputs
        for i, layer in enumerate(self.layers):
            h = layer(graph, h)
            if i < len(self.layers) - 1:  # Not the last layer
                h = {k: F.elu(v.flatten(1)) for k, v in h.items()}
            else:  # Last layer - flatten attention heads
                h = {k: v.flatten(1) for k, v in h.items()}
        
        # Project to output dimension to match Sentence-BERT (384)
        h = {k: self.output_projection(v) for k, v in h.items()}
        
        # Apply batch normalization to prevent embedding collapse
        if apply_batchnorm and self.training:
            h = {k: self.batch_norms[k](v) if k in self.batch_norms else v 
                 for k, v in h.items()}
        
        return h

class GraphEmbeddingTrainer:
    def __init__(self, neo4j_uri="neo4j://127.0.0.1:7687", 
                 neo4j_username="neo4j", 
                 neo4j_password="Jackson050609"):
        """åˆå§‹åŒ–è®­ç»ƒå™¨,è¿æ¥åˆ°Neo4jæ•°æ®åº“"""
        # ä¼˜å…ˆä½¿ç”¨GPU,å¦‚æœä¸å¯ç”¨åˆ™é™çº§åˆ°CPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print(f"ğŸ”§ Using device: {self.device} ({torch.cuda.get_device_name(0)})")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            self.device = torch.device('cpu')
            print(f"ğŸ”§ CUDA not available, falling back to: {self.device}")
        
        # è¿æ¥Neo4j
        print(f"\nğŸ”Œ Connecting to Neo4j at {neo4j_uri}...")
        try:
            self.graph = Graph(neo4j_uri, auth=(neo4j_username, neo4j_password))
            self.graph.run("RETURN 1")
            print("âœ… Connected to Neo4j successfully")
        except Exception as e:
            print(f"âŒ Failed to connect to Neo4j: {e}")
            raise
        
        # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
        print("\nğŸ“ Loading sentence transformer model...")
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Sentence transformer loaded")
        
        # ç¼“å­˜ç›®å½•
        self.cache_dir = "cache"
        os.makedirs(self.cache_dir, exist_ok=True)
        
    def load_data_from_neo4j(self, sample_size=None):
        """ä»Neo4jåŠ è½½å›¾æ•°æ®"""
        print("\nğŸ“Š Loading data from Neo4j...")
        
        # 1. åŠ è½½è®ºæ–‡èŠ‚ç‚¹
        print("\nğŸ“„ Loading papers...")
        if sample_size:
            papers_query = f"""
            MATCH (p:Paper)
            WHERE p.title IS NOT NULL AND p.title <> ''
            RETURN p.paper_id as paper_id, p.title as title, 
                   p.abstract as abstract, p.year as year
            LIMIT {sample_size}
            """
        else:
            papers_query = """
            MATCH (p:Paper)
            WHERE p.title IS NOT NULL AND p.title <> ''
            RETURN p.paper_id as paper_id, p.title as title, 
                   p.abstract as abstract, p.year as year
            """
        papers_data = self.graph.run(papers_query).data()
        papers_df = pd.DataFrame(papers_data)
        print(f"   âœ… Loaded {len(papers_df)} papers")
        
        # 2. åŠ è½½ä½œè€…èŠ‚ç‚¹
        print("\nğŸ‘¥ Loading authors...")
        authors_query = """
        MATCH (a:Author)<-[:WRITTEN_BY]-(p:Paper)
        WHERE p.paper_id IN $paper_ids
        RETURN DISTINCT a.author_id as author_id, a.name as name
        """
        authors_data = self.graph.run(authors_query, paper_ids=papers_df['paper_id'].tolist()).data()
        authors_df = pd.DataFrame(authors_data) if authors_data else pd.DataFrame({'author_id': [], 'name': []})
        print(f"   âœ… Loaded {len(authors_df)} authors")
        
        # 3. åŠ è½½å…³é”®è¯èŠ‚ç‚¹
        print("\nğŸ·ï¸  Loading keywords...")
        keywords_query = """
        MATCH (k:Keyword)<-[:HAS_KEYWORD]-(p:Paper)
        WHERE p.paper_id IN $paper_ids
        RETURN DISTINCT k.keyword_id as keyword_id, k.keyword as keyword
        """
        keywords_data = self.graph.run(keywords_query, paper_ids=papers_df['paper_id'].tolist()).data()
        keywords_df = pd.DataFrame(keywords_data) if keywords_data else pd.DataFrame({'keyword_id': [], 'keyword': []})
        print(f"   âœ… Loaded {len(keywords_df)} keywords")
        
        # 4. åŠ è½½å…³ç³»
        print("\nğŸ”— Loading relationships...")
        
        # WRITTEN_BY å…³ç³»
        written_by_query = """
        MATCH (p:Paper)-[r:WRITTEN_BY]->(a:Author)
        WHERE p.paper_id IN $paper_ids
        RETURN p.paper_id as paper_id, a.author_id as author_id, 
               r.author_order as author_order
        """
        written_by_data = self.graph.run(written_by_query, paper_ids=papers_df['paper_id'].tolist()).data()
        written_by_df = pd.DataFrame(written_by_data) if written_by_data else pd.DataFrame({'paper_id': [], 'author_id': [], 'author_order': []})
        print(f"   âœ… Loaded {len(written_by_df)} WRITTEN_BY relationships")
        
        # CITES å…³ç³»
        cites_query = """
        MATCH (citing:Paper)-[r:CITES]->(cited:Paper)
        WHERE citing.paper_id IN $paper_ids 
          AND cited.paper_id IN $paper_ids
        RETURN citing.paper_id as citing_paper, cited.paper_id as cited_paper
        """
        cites_data = self.graph.run(cites_query, paper_ids=papers_df['paper_id'].tolist()).data()
        cites_df = pd.DataFrame(cites_data) if cites_data else pd.DataFrame({'citing_paper': [], 'cited_paper': []})
        print(f"   âœ… Loaded {len(cites_df)} CITES relationships")
        
        # HAS_KEYWORD å…³ç³»
        has_keyword_query = """
        MATCH (p:Paper)-[r:HAS_KEYWORD]->(k:Keyword)
        WHERE p.paper_id IN $paper_ids
        RETURN p.paper_id as paper_id, k.keyword_id as keyword_id
        """
        has_keyword_data = self.graph.run(has_keyword_query, paper_ids=papers_df['paper_id'].tolist()).data()
        has_keyword_df = pd.DataFrame(has_keyword_data) if has_keyword_data else pd.DataFrame({'paper_id': [], 'keyword_id': []})
        print(f"   âœ… Loaded {len(has_keyword_df)} HAS_KEYWORD relationships")
        
        return {
            'papers': papers_df,
            'authors': authors_df,
            'keywords': keywords_df,
            'written_by': written_by_df,
            'cites': cites_df,
            'has_keyword': has_keyword_df
        }
    
    def prepare_node_features(self, data_dict):
        """ä»Neo4jæ•°æ®ç”ŸæˆèŠ‚ç‚¹ç‰¹å¾"""
        print("\nğŸ¨ Preparing node features...")
        
        papers_df = data_dict['papers']
        authors_df = data_dict['authors']
        keywords_df = data_dict['keywords']
        
        # ä¸ºè®ºæ–‡èŠ‚ç‚¹ç”Ÿæˆæ–‡æœ¬åµŒå…¥ç‰¹å¾
        print("   ğŸ“„ Encoding paper titles and abstracts...")
        paper_texts = []
        for _, row in papers_df.iterrows():
            text = str(row['title'])
            if pd.notna(row['abstract']) and row['abstract']:
                text += " " + str(row['abstract'])
            paper_texts.append(text)
        
        # ä½¿ç”¨sentence transformerç”ŸæˆåµŒå…¥
        paper_embeddings = self.sentence_model.encode(
            paper_texts, 
            show_progress_bar=True,
            batch_size=128
        )
        paper_features = torch.FloatTensor(paper_embeddings)
        print(f"      Papers feature shape: {paper_features.shape}")
        
        # ä¸ºä½œè€…èŠ‚ç‚¹ç”Ÿæˆç‰¹å¾ (ä½¿ç”¨ä½œè€…åçš„åµŒå…¥)
        print("   ğŸ‘¥ Encoding author names...")
        author_names = authors_df['name'].fillna('Unknown').tolist()
        author_embeddings = self.sentence_model.encode(
            author_names,
            show_progress_bar=True,
            batch_size=128
        )
        author_features = torch.FloatTensor(author_embeddings)
        print(f"      Authors feature shape: {author_features.shape}")
        
        # ä¸ºå…³é”®è¯èŠ‚ç‚¹ç”Ÿæˆç‰¹å¾
        print("   ğŸ·ï¸  Encoding keywords...")
        keyword_texts = keywords_df['keyword'].fillna('').tolist()
        keyword_embeddings = self.sentence_model.encode(
            keyword_texts,
            show_progress_bar=True,
            batch_size=128
        )
        keyword_features = torch.FloatTensor(keyword_embeddings)
        print(f"      Keywords feature shape: {keyword_features.shape}")
        
        return {
            'paper': paper_features,
            'author': author_features,
            'keyword': keyword_features
        }
    def build_dgl_heterograph(self, data_dict):
        """ä»Neo4jæ•°æ®æ„å»ºDGLå¼‚è´¨å›¾"""
        print("\nğŸ”¨ Building DGL heterogeneous graph...")
        
        papers_df = data_dict['papers']
        authors_df = data_dict['authors']
        keywords_df = data_dict['keywords']
        written_by_df = data_dict['written_by']
        cites_df = data_dict['cites']
        has_keyword_df = data_dict['has_keyword']
        
        # åˆ›å»ºIDæ˜ å°„ (å­—ç¬¦ä¸²ID -> æ•°å€¼ç´¢å¼•)
        print("   ğŸ”¢ Creating ID mappings...")
        paper_id_map = {pid: idx for idx, pid in enumerate(papers_df['paper_id'])}
        author_id_map = {aid: idx for idx, aid in enumerate(authors_df['author_id'])}
        keyword_id_map = {kid: idx for idx, kid in enumerate(keywords_df['keyword_id'])}
        
        # æ„å»ºå¼‚è´¨å›¾æ•°æ®å­—å…¸
        data_dict_graph = {}
        
        # 1. WRITTEN_BY å…³ç³»: Paper -> Author
        print("   âœï¸  Adding WRITTEN_BY edges...")
        paper_to_author_src = []
        paper_to_author_dst = []
        for _, row in written_by_df.iterrows():
            if row['paper_id'] in paper_id_map and row['author_id'] in author_id_map:
                paper_to_author_src.append(paper_id_map[row['paper_id']])
                paper_to_author_dst.append(author_id_map[row['author_id']])
        
        if paper_to_author_src:
            data_dict_graph[('paper', 'written_by', 'author')] = (
                torch.tensor(paper_to_author_src), 
                torch.tensor(paper_to_author_dst)
            )
            # åå‘è¾¹: Author -> Paper
            data_dict_graph[('author', 'writes', 'paper')] = (
                torch.tensor(paper_to_author_dst),
                torch.tensor(paper_to_author_src)
            )
            print(f"      Added {len(paper_to_author_src)} WRITTEN_BY edges")
        
        # 2. CITES å…³ç³»: Paper -> Paper
        print("   ğŸ“š Adding CITES edges...")
        citing_papers = []
        cited_papers = []
        for _, row in cites_df.iterrows():
            if row['citing_paper'] in paper_id_map and row['cited_paper'] in paper_id_map:
                citing_papers.append(paper_id_map[row['citing_paper']])
                cited_papers.append(paper_id_map[row['cited_paper']])
        
        if citing_papers:
            data_dict_graph[('paper', 'cites', 'paper')] = (
                torch.tensor(citing_papers),
                torch.tensor(cited_papers)
            )
            # åå‘è¾¹: Paper <- Paper (è¢«å¼•ç”¨)
            data_dict_graph[('paper', 'cited_by', 'paper')] = (
                torch.tensor(cited_papers),
                torch.tensor(citing_papers)
            )
            print(f"      Added {len(citing_papers)} CITES edges")
        
        # 3. HAS_KEYWORD å…³ç³»: Paper -> Keyword
        print("   ğŸ·ï¸  Adding HAS_KEYWORD edges...")
        paper_to_keyword_src = []
        paper_to_keyword_dst = []
        for _, row in has_keyword_df.iterrows():
            if row['paper_id'] in paper_id_map and row['keyword_id'] in keyword_id_map:
                paper_to_keyword_src.append(paper_id_map[row['paper_id']])
                paper_to_keyword_dst.append(keyword_id_map[row['keyword_id']])
        
        if paper_to_keyword_src:
            data_dict_graph[('paper', 'has_keyword', 'keyword')] = (
                torch.tensor(paper_to_keyword_src),
                torch.tensor(paper_to_keyword_dst)
            )
            # åå‘è¾¹: Keyword -> Paper
            data_dict_graph[('keyword', 'belongs_to', 'paper')] = (
                torch.tensor(paper_to_keyword_dst),
                torch.tensor(paper_to_keyword_src)
            )
            print(f"      Added {len(paper_to_keyword_src)} HAS_KEYWORD edges")
        
        # åˆ›å»ºå¼‚è´¨å›¾
        print("   ğŸ¯ Creating heterogeneous graph...")
        graph = dgl.heterograph(data_dict_graph)
        
        print(f"\nâœ… Graph created successfully!")
        print(f"   Node types: {graph.ntypes}")
        print(f"   Edge types: {graph.etypes}")
        print(f"   Number of nodes: {dict(zip(graph.ntypes, [graph.num_nodes(ntype) for ntype in graph.ntypes]))}")
        print(f"   Number of edges: {graph.num_edges()}")
        
        return graph, paper_id_map, author_id_map, keyword_id_map
    
    def train_model(self, sample_size=10000, epochs=100, lr=0.001, save_dir='models',
                   hidden_dim=192, out_dim=384, num_heads=4):
        """è®­ç»ƒHANæ¨¡å‹
        
        Args:
            sample_size: é‡‡æ ·è®ºæ–‡æ•°é‡
            epochs: è®­ç»ƒè½®æ•°
            lr: å­¦ä¹ ç‡
            save_dir: ä¿å­˜ç›®å½•
            hidden_dim: éšè—å±‚ç»´åº¦
            out_dim: è¾“å‡ºåµŒå…¥ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
        """
        print("=" * 70)
        print("ğŸš€ Starting HAN Model Training")
        print("=" * 70)
        
        overall_start_time = time.time()
        
        # 1. ä»Neo4jåŠ è½½æ•°æ®
        data_dict = self.load_data_from_neo4j(sample_size=sample_size)
        
        # 2. æ„å»ºå¼‚è´¨å›¾
        graph, paper_id_map, author_id_map, keyword_id_map = self.build_dgl_heterograph(data_dict)
        
        # 3. å‡†å¤‡èŠ‚ç‚¹ç‰¹å¾
        node_features = self.prepare_node_features(data_dict)
        
        in_dim = 384  # Sentence-BERT embedding dimension
        etypes = graph.etypes
        
        model = HANModel(in_dim, hidden_dim, out_dim, num_heads, etypes)
        model = model.to(self.device)
        
        # å°†å›¾å’ŒèŠ‚ç‚¹ç‰¹å¾éƒ½ç§»åˆ°è®¾å¤‡ä¸Š
        graph = graph.to(self.device)
        node_features = {k: v.to(self.device) for k, v in node_features.items()}
        
        print(f"\nğŸ—ï¸  Model Architecture:")
        print(f"   Input Dim: {in_dim}")
        print(f"   Hidden Dim: {hidden_dim}")
        print(f"   Output Dim: {out_dim}")
        print(f"   Attention Heads: {num_heads}")
        print(f"   Edge Types: {len(etypes)}")
        print(f"   Model Device: {self.device}")
        print(f"   Graph Device: {self.device}")
        
        print(f"\nğŸ”§ Trial 3 Improvements:")
        print(f"   âœ… Batch Normalization: Prevents embedding collapse")
        print(f"   âœ… Contrastive Loss: Encourages embedding diversity")
        print(f"   âœ… Increased Regularization: 100x for author, 80x for keyword")
        print(f"   âœ… Gradient Accumulation: Compensates for smaller batch size")
        print(f"   âœ… Smaller Model: 96D hidden vs 128D (25% less memory)")
        
        # 5. å®šä¹‰ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        # è®¾ç½®æ‰¹å¤„ç†å‚æ•°
        batch_size = 128
        gradient_accumulation_steps = 1
        print(f"\nğŸ’¾ Memory-Efficient Settings:")
        print(f"   Batch Size: {batch_size} nodes per batch")
        print(f"   Gradient Accumulation Steps: {gradient_accumulation_steps}")
        
        # 7. è®­ç»ƒå¾ªç¯
        print(f"\nğŸ¯ Training for {epochs} epochs...")
        print("=" * 70 + "\n")
        
        model.train()
        train_start_time = time.time()
        
        # è®°å½•è®­ç»ƒå†å²
        training_history = {
            'losses': [],
            'epoch_times': [],
            'checkpoints': []  # æ¯10æ­¥çš„è¯¦ç»†è®°å½•
        }
        
        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(range(epochs), 
                   desc="Training", 
                   ncols=100,
                   leave=True,
                   dynamic_ncols=True,
                   bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}')
        
        for epoch in pbar:
            epoch_start = time.time()
            
            # æ¸…ç©ºGPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            optimizer.zero_grad()
            
            try:
                # å‰å‘ä¼ æ’­ï¼ˆä»…è®¡ç®—æ¨¡å‹ï¼Œå›¾ä¿æŒåœ¨CPUï¼‰
                embeddings = model(graph, node_features, apply_batchnorm=False)
                
                # è®¡ç®—æ”¹è¿›çš„æŸå¤±å‡½æ•° - é˜²æ­¢ embedding collapse
                # æ¿€è¿›çš„æ­£åˆ™åŒ–ç­–ç•¥ï¼šå¼ºåˆ¶åˆ†æ•£åµŒå…¥ï¼Œé™ä½èŠ‚ç‚¹ç›¸ä¼¼åº¦
                node_weights = {
                    'paper': 0.01,        # è®ºæ–‡èŠ‚ç‚¹
                    'author': 0.02,       # ä½œè€…èŠ‚ç‚¹ - å¢åŠ 2å€
                    'keyword': 0.02       # å…³é”®è¯èŠ‚ç‚¹ - å¢åŠ 2å€
                }
                
                loss = 0
                num_nodes = 0
                
                for ntype, emb in embeddings.items():
                    emb = emb.to(self.device)
                    weight = node_weights.get(ntype, 0.0001)
                    
                    # 1. æ–¹å·®æœ€å¤§åŒ– - ç¡®ä¿åµŒå…¥æœ‰å˜å¼‚æ€§ï¼Œé˜²æ­¢ collapse
                    embedding_variance = torch.mean(torch.var(emb, dim=0))
                    # ç›®æ ‡æ–¹å·®æé«˜åˆ°0.01ï¼Œæ›´æ¿€è¿›åœ°å¼ºåˆ¶åˆ†æ•£
                    variance_penalty = F.relu(0.01 - embedding_variance)
                    loss += variance_penalty * weight * 100
                    
                    # 2. å…¨å±€å¯¹æ¯”æŸå¤± - å¼ºåˆ¶æ‰€æœ‰èŠ‚ç‚¹ç›¸ä¼¼åº¦é™ä½ï¼ˆä¸é‡‡æ ·ï¼Œå…¨è®¡ç®—ï¼‰
                    if emb.shape[0] > 1:
                        # å¯¹äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œåˆ†æ‰¹è®¡ç®—ç›¸ä¼¼åº¦ä»¥é¿å…OOM
                        if emb.shape[0] > 5000:
                            # é‡‡ç”¨æ›´å¤§çš„æ ·æœ¬ (2000) ç¡®ä¿è¦†ç›–è¶³å¤Ÿçš„èŠ‚ç‚¹å¯¹
                            sample_size = 2000
                            indices = torch.randperm(emb.shape[0])[:sample_size]
                            sampled_emb = emb[indices]
                        else:
                            # å°æ•°æ®é›†è®¡ç®—å…¨éƒ¨
                            sampled_emb = emb
                        
                        normalized = F.normalize(sampled_emb, p=2, dim=1)
                        similarity_matrix = torch.mm(normalized, normalized.t())
                        
                        mask = torch.eye(similarity_matrix.shape[0], device=self.device, dtype=torch.bool)
                        masked_sim = similarity_matrix.masked_fill(mask, 0)
                        
                        # æ›´æ¿€è¿›çš„ç›®æ ‡ï¼šç›¸ä¼¼åº¦å¿…é¡» < 0.15ï¼ˆè€Œä¸æ˜¯0.3ï¼‰
                        # è¿™ä¼šå¼ºåˆ¶èŠ‚ç‚¹æ›´åˆ†æ•£
                        contrast_loss = F.relu(masked_sim.abs() - 0.15).mean()
                        # å¤§å¹…å¢åŠ æƒé‡ç¡®ä¿å¯¹æ¯”æŸå¤±ä¸»å¯¼è®­ç»ƒ
                        loss += contrast_loss * weight * 500
                    
                    num_nodes += emb.shape[0]
                
                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¯ gradient_accumulation_steps æ­¥è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°
                if (epoch + 1) % gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    optimizer.zero_grad()
                
                epoch_time = time.time() - epoch_start
                loss_value = loss.item() * gradient_accumulation_steps  # è¿˜åŸå®é™…æŸå¤±ç”¨äºæ˜¾ç¤º
                
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    print(f"\nâš ï¸  GPU OOM detected. Reducing batch in next epoch...")
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    # ç»§ç»­è®­ç»ƒï¼Œä¸‹ä¸€è½®ä¼šå°è¯•æ¢å¤
                    epoch_time = time.time() - epoch_start
                    loss_value = float('nan')
                else:
                    raise
            
            training_history['losses'].append(loss_value)
            training_history['epoch_times'].append(epoch_time)
            
            # æ¯10ä¸ªepochè®°å½•è¯¦ç»†ä¿¡æ¯(ä½†ä¸æ‰“å°,åªè®°å½•)
            if (epoch + 1) % 10 == 0:
                checkpoint = {
                    'epoch': epoch + 1,
                    'loss': loss_value,
                    'avg_loss_last_10': np.mean([l for l in training_history['losses'][-10:] if not np.isnan(l)]),
                    'epoch_time': epoch_time,
                    'avg_epoch_time': np.mean(training_history['epoch_times'][-10:]),
                    'elapsed_time': time.time() - train_start_time
                }
                training_history['checkpoints'].append(checkpoint)
            
            # è®¡ç®—ETA
            if len(training_history['epoch_times']) > 0:
                avg_epoch_time = np.mean(training_history['epoch_times'])
                remaining_epochs = epochs - (epoch + 1)
                eta_seconds = avg_epoch_time * remaining_epochs
                eta = timedelta(seconds=int(eta_seconds))
            else:
                eta = timedelta(seconds=0)
            
            # æ›´æ–°è¿›åº¦æ¡(åœ¨åŒä¸€è¡Œ)
            pbar.set_postfix({
                'Loss': f'{loss_value:.4f}' if not np.isnan(loss_value) else 'OOM',
                'Time': f'{epoch_time:.2f}s',
                'ETA': str(eta)
            })
        
        pbar.close()  # ç¡®ä¿è¿›åº¦æ¡æ­£ç¡®å…³é—­
        
        total_train_time = time.time() - train_start_time
        print(f"\nâœ… Training completed in {timedelta(seconds=int(total_train_time))}!")
        
        # 8. ä¿å­˜æ¨¡å‹å’ŒåµŒå…¥
        print(f"\nğŸ’¾ Saving to {save_dir}/...")
        os.makedirs(save_dir, exist_ok=True)
        
        # è·å–æœ€ç»ˆåµŒå…¥
        model.eval()
        with torch.no_grad():
            final_embeddings = model(graph, node_features)
            final_embeddings = {k: v.cpu() for k, v in final_embeddings.items()}
        
        # ä¿å­˜æ¨¡å‹æ–‡ä»¶
        save_data = {
            'model_state_dict': model.state_dict(),
            'embeddings': final_embeddings,
            'id_maps': {
                'paper': paper_id_map,
                'author': author_id_map,
                'keyword': keyword_id_map
            },
            'config': {
                'in_dim': in_dim,
                'hidden_dim': hidden_dim,
                'out_dim': out_dim,
                'num_heads': num_heads,
                'etypes': etypes
            }
        }
        
        model_path = os.path.join(save_dir, 'han_embeddings.pth')
        torch.save(save_data, model_path)
        print(f"   âœ… Model saved: {model_path}")
        
        # ç”Ÿæˆè®­ç»ƒæ‘˜è¦
        summary = {
            'training_config': {
                'sample_size': sample_size,
                'epochs': epochs,
                'learning_rate': lr,
                'device': str(self.device),
                'save_directory': save_dir
            },
            'model_architecture': {
                'input_dim': in_dim,
                'hidden_dim': hidden_dim,
                'output_dim': out_dim,
                'num_heads': num_heads,
                'num_edge_types': len(etypes)
            },
            'dataset_info': {
                'num_papers': len(data_dict['papers']),
                'num_authors': len(data_dict['authors']),
                'num_keywords': len(data_dict['keywords']),
                'num_written_by': len(data_dict['written_by']),
                'num_cites': len(data_dict['cites']),
                'num_has_keyword': len(data_dict['has_keyword'])
            },
            'training_statistics': {
                'total_time_seconds': total_train_time,
                'total_time_formatted': str(timedelta(seconds=int(total_train_time))),
                'average_epoch_time': np.mean(training_history['epoch_times']),
                'final_loss': training_history['losses'][-1],
                'min_loss': min(training_history['losses']),
                'max_loss': max(training_history['losses']),
                'avg_loss': np.mean(training_history['losses'])
            },
            'checkpoints': training_history['checkpoints'],
            'loss_history': training_history['losses'],
            'embedding_stats': {
                ntype: {
                    'shape': list(emb.shape),
                    'mean': float(emb.mean()),
                    'std': float(emb.std()),
                    'min': float(emb.min()),
                    'max': float(emb.max())
                }
                for ntype, emb in final_embeddings.items()
            },
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ä¿å­˜æ‘˜è¦æ–‡ä»¶
        summary_path = os.path.join(save_dir, 'summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"   âœ… Summary saved: {summary_path}")
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        print("\nğŸ“Š Training Summary:")
        print(f"   Final Loss: {training_history['losses'][-1]:.4f}")
        print(f"   Best Loss: {min(training_history['losses']):.4f}")
        print(f"   Avg Loss: {np.mean(training_history['losses']):.4f}")
        print(f"   Total Time: {timedelta(seconds=int(total_train_time))}")
        
        return model, graph, final_embeddings, save_data['id_maps']

if __name__ == "__main__":
    import sys

    # ===================== DEBUG CONFIGURATION =====================
    DEBUG_MODE = True  # è®¾ç½®ä¸º True å¯ç›´æ¥åœ¨æ­¤å¤„è°ƒè¯•å‚æ•°
    # ä»…åœ¨ DEBUG_MODE=True æ—¶ç”Ÿæ•ˆ
    DEBUG_CONFIG = {
        'NEO4J_URI': "neo4j://127.0.0.1:7687",
        'NEO4J_USERNAME': "neo4j",
        'NEO4J_PASSWORD': "87654321",
        'SAMPLE_SIZE': 10000,
        'EPOCHS': 500,
        'LEARNING_RATE': 0.001,
        'SAVE_DIR': 'models/trial4',
        'HIDDEN_DIM': 128,
        'OUT_DIM': 384,
        'NUM_HEADS': 16
    }
    # ===================== END DEBUG CONFIGURATION =================

    print("=" * 70)
    print("ğŸ“ Academic Graph HAN Model Training")
    print("=" * 70)

    if DEBUG_MODE:
        NEO4J_URI = DEBUG_CONFIG['NEO4J_URI']
        NEO4J_USERNAME = DEBUG_CONFIG['NEO4J_USERNAME']
        NEO4J_PASSWORD = DEBUG_CONFIG['NEO4J_PASSWORD']
        SAMPLE_SIZE = DEBUG_CONFIG['SAMPLE_SIZE']
        EPOCHS = DEBUG_CONFIG['EPOCHS']
        LEARNING_RATE = DEBUG_CONFIG['LEARNING_RATE']
        SAVE_DIR = DEBUG_CONFIG['SAVE_DIR']
        HIDDEN_DIM = DEBUG_CONFIG['HIDDEN_DIM']
        OUT_DIM = DEBUG_CONFIG['OUT_DIM']
        NUM_HEADS = DEBUG_CONFIG['NUM_HEADS']
    else:
        if len(sys.argv) != 11:
            print("âŒ å‚æ•°æ•°é‡é”™è¯¯ï¼è¯·é€šè¿‡ run.sh æˆ–å‘½ä»¤è¡Œå®Œæ•´ä¼ é€’ 10 ä¸ªå‚æ•°ï¼š")
            print("python han_model.py <NEO4J_URI> <NEO4J_USERNAME> <NEO4J_PASSWORD> <SAMPLE_SIZE> <EPOCHS> <LEARNING_RATE> <SAVE_DIR> <HIDDEN_DIM> <OUT_DIM> <NUM_HEADS>")
            exit(1)
        NEO4J_URI = sys.argv[1]
        NEO4J_USERNAME = sys.argv[2]
        NEO4J_PASSWORD = sys.argv[3]
        SAMPLE_SIZE = int(sys.argv[4]) if sys.argv[4] != "None" else None
        EPOCHS = int(sys.argv[5])
        LEARNING_RATE = float(sys.argv[6])
        SAVE_DIR = sys.argv[7]
        HIDDEN_DIM = int(sys.argv[8])
        OUT_DIM = int(sys.argv[9])
        NUM_HEADS = int(sys.argv[10])

    print(f"\nâš™ï¸  Configuration:")
    print(f"   Neo4j URI: {NEO4J_URI}")
    print(f"   Sample size: {SAMPLE_SIZE if SAMPLE_SIZE else 'All'} papers")
    print(f"   Model: HIDDEN={HIDDEN_DIM}, OUT={OUT_DIM}, HEADS={NUM_HEADS}")
    print(f"   Training: EPOCHS={EPOCHS}, LR={LEARNING_RATE}")
    print(f"   Save directory: {SAVE_DIR}")

    try:
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = GraphEmbeddingTrainer(
            neo4j_uri=NEO4J_URI,
            neo4j_username=NEO4J_USERNAME,
            neo4j_password=NEO4J_PASSWORD
        )

        # è®­ç»ƒæ¨¡å‹
        model, graph, embeddings, id_maps = trainer.train_model(
            sample_size=SAMPLE_SIZE,
            epochs=EPOCHS,
            lr=LEARNING_RATE,
            save_dir=SAVE_DIR,
            hidden_dim=HIDDEN_DIM,
            out_dim=OUT_DIM,
            num_heads=NUM_HEADS
        )

        print("\n" + "=" * 70)
        print("âœ… All tasks completed successfully!")
        print("=" * 70)

        print(f"\nğŸ“ Output files:")
        print(f"   â€¢ {SAVE_DIR}/han_embeddings.pth - Model and embeddings")
        print(f"   â€¢ {SAVE_DIR}/summary.json - Training summary and statistics")

    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ Troubleshooting:")
        print("   1. Check Neo4j is running and accessible")
        print("   2. Verify data exists in Neo4j database")
        print("   3. Install required packages: pip install torch dgl sentence-transformers py2neo")
        print("   4. Reduce SAMPLE_SIZE if out of memory")
        exit(1)