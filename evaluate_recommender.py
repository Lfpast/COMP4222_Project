# [file name]: evaluate_recommender_fixed.py
import torch
import numpy as np
import pandas as pd
from sklearn.metrics import precision_score, recall_score, ndcg_score
import json
import os
from typing import List, Dict, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

class RecommenderEvaluator:
    def __init__(self, 
                 recommender_system,
                 test_data_path: str = None,
                 k_values: List[int] = [5, 10, 20],
                 evaluation_strategy: str = "filtered"):
        """
        åˆå§‹åŒ–æ¨èç³»ç»Ÿè¯„ä¼°å™¨
        
        Args:
            evaluation_strategy: è¯„ä¼°ç­–ç•¥
                - "filtered": åªç”¨æ•°æ®åº“ä¸­å­˜åœ¨çš„ground truthè®¡ç®—æŒ‡æ ‡ï¼ˆæ¨èï¼‰
                - "full": ç”¨å…¨éƒ¨ground truthè®¡ç®—æŒ‡æ ‡ï¼ˆä¼šæ˜¾ç¤ºåä½ä½†å¯çœ‹å‡ºæ•°æ®ç¼ºå¤±ç¨‹åº¦ï¼‰
        """
        self.recommender = recommender_system
        self.k_values = k_values
        self.test_data = None
        self.ground_truth = {}
        self.evaluation_strategy = evaluation_strategy
        
        if test_data_path and os.path.exists(test_data_path):
            self.load_test_data(test_data_path)
    
    def load_test_data(self, test_data_path: str):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“Š Loading test data from {test_data_path}...")
        try:
            # æ”¯æŒå¤šç§æ ¼å¼ï¼šCSV, JSONç­‰
            if test_data_path.endswith('.csv'):
                self.test_data = pd.read_csv(test_data_path)
            elif test_data_path.endswith('.jsonl') or test_data_path.endswith('.ndjson'):
                # line-delimited JSON
                self.test_data = pd.read_json(test_data_path, lines=True)
            elif test_data_path.endswith('.json'):
                with open(test_data_path, 'r', encoding='utf-8') as f:
                    # support lists of objects or dict-of-lists
                    raw = json.load(f)
                    if isinstance(raw, list):
                        self.test_data = pd.DataFrame(raw)
                    else:
                        self.test_data = pd.DataFrame(raw)
            print(f"âœ… Loaded {len(self.test_data)} test samples")
        except Exception as e:
            print(f"âŒ Failed to load test data: {e}")
            self.test_data = None
    
    def prepare_ground_truth(self, citation_data: Dict[str, List[str]] = None):
        """
        å‡†å¤‡ground truthæ•°æ® - ä¿®å¤IDæ ¼å¼é—®é¢˜
        """
        if citation_data:
            self.ground_truth = citation_data
        elif self.test_data is not None:
            # ä»æµ‹è¯•æ•°æ®ä¸­æå–ground truthï¼Œç¡®ä¿IDæ ¼å¼ä¸€è‡´
            cols = set(self.test_data.columns)
            for _, row in self.test_data.iterrows():
                # paper id may be under 'paper_id' or 'id' depending on dataset
                if 'paper_id' in cols and pd.notna(row.get('paper_id')):
                    paper_id = str(row['paper_id']).strip()
                elif 'id' in cols and pd.notna(row.get('id')):
                    paper_id = str(row['id']).strip()
                else:
                    # skip rows without identifiable id
                    continue

                # å¤„ç†ä¸åŒçš„ground truthæ ¼å¼: try several common field names
                citations = []
                gt_citations = None
                for key in ('ground_truth_citations', 'references', 'citations'):
                    if key in cols:
                        raw = row.get(key)
                        # skip null/NaN
                        if raw is None:
                            continue
                        try:
                            if isinstance(raw, float) and np.isnan(raw):
                                continue
                        except Exception:
                            pass
                        gt_citations = raw
                        break

                if gt_citations is None:
                    citations = []
                elif isinstance(gt_citations, str):
                    # sometimes string-encoded lists or comma-separated ids
                    try:
                        parsed = json.loads(gt_citations)
                        if isinstance(parsed, list):
                            citations = parsed
                        else:
                            citations = [str(parsed)]
                    except Exception:
                        # fallback to eval or comma-split
                        try:
                            citations = eval(gt_citations)
                        except Exception:
                            citations = [x.strip() for x in str(gt_citations).split(',') if x.strip()]
                elif isinstance(gt_citations, list):
                    citations = gt_citations
                else:
                    citations = [str(gt_citations)]

                # ç¡®ä¿æ‰€æœ‰citation IDéƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                self.ground_truth[paper_id] = [str(cite).strip() for cite in citations if cite]
        
        print(f"âœ… Prepared ground truth for {len(self.ground_truth)} papers")
        print(f"   Sample ground truth: {list(self.ground_truth.items())[:2]}")
    
    def _check_paper_in_embeddings(self, paper_id: str) -> bool:
        """æ£€æŸ¥è®ºæ–‡æ˜¯å¦åœ¨åµŒå…¥ä¸­"""
        return paper_id in self.recommender.id_maps['paper']
    
    def _get_available_test_papers(self) -> List[str]:
        """è·å–åœ¨åµŒå…¥ä¸­å¯ç”¨çš„æµ‹è¯•è®ºæ–‡"""
        if not self.ground_truth:
            return []
        
        available_papers = []
        for paper_id in self.ground_truth.keys():
            if self._check_paper_in_embeddings(paper_id):
                available_papers.append(paper_id)
        
        print(f"ğŸ“‹ Available test papers in embeddings: {len(available_papers)}/{len(self.ground_truth)}")
        return available_papers
    
    def evaluate_single_paper(self, paper_id: str, method: str = "collaborative") -> Dict[str, Any]:
        """
        è¯„ä¼°å•ç¯‡è®ºæ–‡çš„æ¨èæ•ˆæœ - ä¿®å¤ç‰ˆæœ¬
        """
        # æ£€æŸ¥è®ºæ–‡æ˜¯å¦åœ¨åµŒå…¥ä¸­
        if not self._check_paper_in_embeddings(paper_id):
            return {"error": f"Paper {paper_id} not in embeddings", "paper_id": paper_id}
        
        if paper_id not in self.ground_truth:
            return {"error": f"No ground truth for paper {paper_id}", "paper_id": paper_id}
        
        true_citations = set(self.ground_truth[paper_id])
        
        # è·å–æ¨èç»“æœ
        try:
            if method == "collaborative":
                recommendations = self.recommender.enhanced_collaborative_recommendation(
                    paper_id, top_k=max(self.k_values)
                )
            elif method == "content":
                # ä½¿ç”¨è®ºæ–‡æ ‡é¢˜è¿›è¡ŒåŸºäºå†…å®¹çš„æ¨è
                paper_metadata = self.recommender.get_paper_metadata([paper_id])
                title = paper_metadata.get(paper_id, {}).get('title', '')
                if title:
                    recommendations = self.recommender.content_based_paper_recommendation(
                        title, top_k=max(self.k_values)
                    )
                else:
                    return {"error": f"No title available for paper {paper_id}", "paper_id": paper_id}
            elif method == "hybrid":
                hybrid_result = self.recommender.optimized_hybrid_recommendation(
                    target_paper_id=paper_id, top_k=max(self.k_values)
                )
                recommendations = hybrid_result['recommendations']
            else:
                return {"error": f"Unknown method: {method}", "paper_id": paper_id}
        except Exception as e:
            return {"error": f"Recommendation failed: {e}", "paper_id": paper_id}
        
        if not recommendations:
            return {"error": f"No recommendations generated for paper {paper_id}", "paper_id": paper_id}
        
        # æå–æ¨èçš„è®ºæ–‡ID - ç¡®ä¿æ ¼å¼ä¸€è‡´
        recommended_ids = [str(rec['paper_id']).strip() for rec in recommendations]
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        results = {
            "paper_id": paper_id,
            "method": method,
            "true_citations_count": len(true_citations),
            "recommendations_count": len(recommended_ids),
            "recommended_ids": recommended_ids[:10]  # ä¿å­˜å‰10ä¸ªæ¨èç”¨äºè°ƒè¯•
        }
        
        # ä¸ºæ¯ä¸ªKå€¼è®¡ç®—æŒ‡æ ‡
        for k in self.k_values:
            k_recs = recommended_ids[:k]
            k_results = self._compute_metrics_at_k(k_recs, true_citations, k)
            results.update({f"{metric}_at_{k}": value for metric, value in k_results.items()})
        
        # æ·»åŠ coverageä¿¡æ¯åˆ°ç»“æœä¸­
        if true_citations:
            available_gt = {gt for gt in true_citations 
                           if gt in self.recommender.id_maps['paper']}
            results['ground_truth_total'] = len(true_citations)
            results['ground_truth_available'] = len(available_gt)
            results['ground_truth_coverage'] = len(available_gt) / len(true_citations)
        
        # è°ƒè¯•ä¿¡æ¯
        if len(true_citations) > 0:
            hits = sum(1 for rec in recommended_ids[:10] if rec in true_citations)
            print(f"   Paper {paper_id}: {hits} hits in top-10, {len(true_citations)} ground truth")
        
        return results
    
    def _compute_metrics_at_k(self, recommendations: List[str], ground_truth: set, k: int) -> Dict[str, float]:
        """åœ¨ç‰¹å®šKå€¼ä¸‹è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        æ”¯æŒä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼š
        - filtered: åªç”¨æ•°æ®åº“ä¸­å­˜åœ¨çš„ground truthè®¡ç®—
        - full: ç”¨å…¨éƒ¨ground truthè®¡ç®—
        """
        # å¤„ç†ä¸¤ç§è¯„ä¼°ç­–ç•¥
        if self.evaluation_strategy == "filtered":
            # åªä¿ç•™åœ¨embeddingsä¸­å­˜åœ¨çš„ground truth
            available_gt = {gt for gt in ground_truth 
                           if gt in self.recommender.id_maps['paper']}
            coverage = len(available_gt) / len(ground_truth) if ground_truth else 0
        else:  # full strategy
            available_gt = ground_truth
            coverage = 1.0
        
        if not available_gt:
            return {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "hit_rate": 0.0,
                "ndcg": 0.0,
                "coverage": coverage
            }
        
        # è®¡ç®—å‘½ä¸­æƒ…å†µ
        hits = [1 if rec in available_gt else 0 for rec in recommendations]
        num_hits = sum(hits)
        
        # Precision@K
        precision = num_hits / k if k > 0 else 0.0
        
        # Recall@K
        recall = num_hits / len(available_gt) if available_gt else 0.0
        
        # F1@K
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # Hit Rate@K
        hit_rate = 1.0 if num_hits > 0 else 0.0
        
        # NDCG@K
        ndcg = self._compute_ndcg(hits, available_gt, k)
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "hit_rate": hit_rate,
            "ndcg": ndcg,
            "coverage": coverage  # æ•°æ®åº“è¦†ç›–ç‡
        }
    
    def _compute_ndcg(self, hits: List[int], ground_truth: set, k: int) -> float:
        """è®¡ç®—NDCG"""
        def dcg(scores):
            return sum(score / np.log2(i + 2) for i, score in enumerate(scores))
        
        # å®é™…DCG
        actual_dcg = dcg(hits)
        
        # ç†æƒ³DCG - æ‰€æœ‰ç›¸å…³é¡¹åœ¨å‰
        ideal_ranking = [1] * min(len(ground_truth), k) + [0] * max(0, k - len(ground_truth))
        ideal_dcg = dcg(ideal_ranking)
        
        return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0.0
    
    def evaluate_batch(self, paper_ids: List[str] = None, method: str = "collaborative") -> Dict[str, Any]:
        """
        æ‰¹é‡è¯„ä¼°å¤šç¯‡è®ºæ–‡ - ä¿®å¤ç‰ˆæœ¬
        """
        if paper_ids is None:
            paper_ids = self._get_available_test_papers()
        
        if not paper_ids:
            return {"error": "No available test papers in embeddings"}
        
        print(f"ğŸ” Evaluating {len(paper_ids)} papers using {method} method...")
        
        results = []
        successful_evals = 0
        error_details = []
        
        for i, paper_id in enumerate(paper_ids):
            if i % 10 == 0:
                print(f"   Progress: {i}/{len(paper_ids)}")
            
            result = self.evaluate_single_paper(paper_id, method)
            if "error" not in result:
                results.append(result)
                successful_evals += 1
            else:
                error_details.append(result)
        
        print(f"âœ… Completed evaluation of {successful_evals} papers")
        print(f"   Errors: {len(error_details)}")
        
        if not results:
            return {
                "error": "No successful evaluations", 
                "error_details": error_details
            }
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._compute_average_metrics(results)
        
        return {
            "method": method,
            "total_papers": len(paper_ids),
            "successful_evaluations": successful_evals,
            "error_details": error_details,
            "average_metrics": avg_metrics,
            "detailed_results": results
        }
    
    def _compute_average_metrics(self, results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        avg_metrics = {}
        
        for k in self.k_values:
            metrics_at_k = {
                "precision": [],
                "recall": [], 
                "f1": [],
                "hit_rate": [],
                "ndcg": [],
                "coverage": []
            }
            
            for result in results:
                for metric in metrics_at_k.keys():
                    value = result.get(f"{metric}_at_{k}", 0)
                    metrics_at_k[metric].append(value)
            
            # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
            avg_metrics[k] = {
                metric: {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values)
                }
                for metric, values in metrics_at_k.items()
            }
        
        return avg_metrics
    
    def compare_methods(self, paper_ids: List[str] = None, methods: List[str] = None) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸åŒæ¨èæ–¹æ³•çš„æ€§èƒ½
        """
        if methods is None:
            methods = ["collaborative", "content", "hybrid"]
        
        comparison_results = {}
        
        for method in methods:
            print(f"\nğŸ“Š Evaluating {method} method...")
            results = self.evaluate_batch(paper_ids, method)
            comparison_results[method] = results
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        comparison_report = self._generate_comparison_report(comparison_results)
        
        return {
            "comparison_results": comparison_results,
            "comparison_report": comparison_report
        }
    
    def _generate_comparison_report(self, comparison_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ–¹æ³•æ¯”è¾ƒæŠ¥å‘Š"""
        report = {}
        
        for k in self.k_values:
            report[f"K={k}"] = {}
            for method, results in comparison_results.items():
                if "average_metrics" in results and k in results["average_metrics"]:
                    report[f"K={k}"][method] = results["average_metrics"][k]
        
        return report
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ - ä¿®å¤è·¯å¾„é—®é¢˜"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            # è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼
            serializable_results = self._make_serializable(results)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ Results saved to {output_path}")
        except Exception as e:
            print(f"âŒ Failed to save results: {e}")
            # å›é€€åˆ°å½“å‰ç›®å½•
            fallback_path = "evaluation_results_fallback.json"
            with open(fallback_path, 'w', encoding='utf-8') as f:
                json.dump(self._make_serializable(results), f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Results saved to {fallback_path} as fallback")
    
    def _make_serializable(self, obj):
        """ç¡®ä¿å¯¹è±¡å¯JSONåºåˆ—åŒ–"""
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return str(obj)

def analyze_data_issues(recommender, test_data_path: str):
    """åˆ†ææ•°æ®é—®é¢˜"""
    print("\nğŸ” Analyzing data issues...")
    
    # æ£€æŸ¥æµ‹è¯•æ•°æ®
    # reuse loading logic: support csv, json, jsonl
    try:
        if test_data_path.endswith('.csv'):
            test_data = pd.read_csv(test_data_path)
        elif test_data_path.endswith('.jsonl') or test_data_path.endswith('.ndjson'):
            test_data = pd.read_json(test_data_path, lines=True)
        elif test_data_path.endswith('.json'):
            with open(test_data_path, 'r', encoding='utf-8') as f:
                raw = json.load(f)
                test_data = pd.DataFrame(raw)
        else:
            test_data = pd.read_csv(test_data_path)
        print(f"ğŸ“Š Test data: {len(test_data)} samples")
    except Exception as e:
        print(f"âŒ Failed to load test data for analysis: {e}")
        return {"common_papers": 0, "missing_papers": 0, "coverage": 0}
    
    # æ£€æŸ¥IDæ˜ å°„
    paper_ids_in_embeddings = set(recommender.id_maps['paper'].keys())
    # test file may use 'paper_id' or 'id'
    if 'paper_id' in test_data.columns:
        paper_ids_in_test = set(str(pid).strip() for pid in test_data['paper_id'] if pd.notna(pid))
    elif 'id' in test_data.columns:
        paper_ids_in_test = set(str(pid).strip() for pid in test_data['id'] if pd.notna(pid))
    else:
        # fallback: try to use first column as id
        first_col = test_data.columns[0]
        paper_ids_in_test = set(str(pid).strip() for pid in test_data[first_col] if pd.notna(pid))
    
    common_papers = paper_ids_in_embeddings & paper_ids_in_test
    missing_papers = paper_ids_in_test - paper_ids_in_embeddings
    
    print(f"ğŸ“‹ Papers in both embeddings and test data: {len(common_papers)}")
    print(f"âŒ Papers in test data but not in embeddings: {len(missing_papers)}")
    
    if missing_papers:
        print(f"   Sample missing papers: {list(missing_papers)[:5]}")
    
    return {
        "common_papers": len(common_papers),
        "missing_papers": len(missing_papers),
        "coverage": len(common_papers) / len(paper_ids_in_test) if paper_ids_in_test else 0
    }

def main():
    """ä¸»è¯„ä¼°å‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    print("ğŸ¯ Starting Recommender System Evaluation (Fixed Version)")
    print("=" * 70)
    
    try:
        # åˆå§‹åŒ–æ¨èç³»ç»Ÿ
        from recommender_system import AcademicRecommender
        
        print("ğŸ“¥ Initializing recommender system...")
        recommender = AcademicRecommender()
        
        # æ”¯æŒä¸¤ç§è¯„ä¼°ç­–ç•¥
        print("\nğŸ”„ Evaluation Strategy Options:")
        print("   1. 'filtered': åªç”¨æ•°æ®åº“ä¸­å­˜åœ¨çš„ground truthè®¡ç®—ï¼ˆæ¨èï¼‰")
        print("   2. 'full': ç”¨å…¨éƒ¨ground truthè®¡ç®—ï¼ˆæ˜¾ç¤ºçœŸå®æ•°æ®è¦†ç›–æƒ…å†µï¼‰")
        
        # åˆ›å»ºè¯„ä¼°å™¨ - é»˜è®¤ä½¿ç”¨filteredç­–ç•¥
        evaluator = RecommenderEvaluator(
            recommender, 
            k_values=[5, 10, 20],
            evaluation_strategy="filtered"  # å¯æ”¹ä¸º"full"æŸ¥çœ‹æ•°æ®ç¼ºå¤±å½±å“
        )
        
        print(f"\nğŸ“Š Using evaluation strategy: {evaluator.evaluation_strategy}")
        
        # æ£€æŸ¥æµ‹è¯•æ•°æ®
        test_data_path = "test_data.csv"  # æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®ground truthæ–‡ä»¶
        if not os.path.exists(test_data_path):
            print(f"âŒ Test data file not found: {test_data_path}")
            print("ğŸ’¡ Please provide your ground truth data in the correct format")
            return
        
        # åˆ†ææ•°æ®é—®é¢˜
        data_analysis = analyze_data_issues(recommender, test_data_path)
        
        if data_analysis["coverage"] < 0.5:
            print(f"âš ï¸ Low coverage: only {data_analysis['coverage']:.1%} of test papers are in embeddings")
            print("ğŸ’¡ Consider retraining the model with more comprehensive data")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        evaluator.load_test_data(test_data_path)
        evaluator.prepare_ground_truth()
        
        # åªè¯„ä¼°åœ¨åµŒå…¥ä¸­å¯ç”¨çš„è®ºæ–‡
        available_papers = evaluator._get_available_test_papers()
        
        if not available_papers:
            print("âŒ No test papers available in embeddings. Evaluation cannot proceed.")
            return
        
        print(f"ğŸ“ Using {len(available_papers)} available papers for evaluation")
        
        # è¯„ä¼°å•ä¸ªæ–¹æ³•
        print("\n" + "=" * 70)
        print("ğŸ“Š Evaluating Collaborative Filtering Method")
        print("=" * 70)
        
        collaborative_results = evaluator.evaluate_batch(available_papers[:20], "collaborative")
        
        if "average_metrics" in collaborative_results:
            print("\nğŸ“ˆ Collaborative Filtering Results:")
            print(f"   Evaluation Strategy: {evaluator.evaluation_strategy}")
            for k, metrics in collaborative_results["average_metrics"].items():
                print(f"  K={k}:")
                for metric, stats in metrics.items():
                    print(f"    {metric}: {stats['mean']:.4f} Â± {stats['std']:.4f}")
            
            # æ˜¾ç¤ºæ•°æ®è¦†ç›–ç‡
            if 'coverage' in collaborative_results["average_metrics"].get(list(collaborative_results["average_metrics"].keys())[0], {}):
                avg_coverage = np.mean([
                    collaborative_results["average_metrics"][k]['coverage']['mean']
                    for k in collaborative_results["average_metrics"].keys()
                ])
                print(f"\n  ğŸ“Š Average Ground Truth Coverage: {avg_coverage:.1%}")
        
        # æ¯”è¾ƒä¸åŒæ–¹æ³•
        print("\n" + "=" * 70)
        print("ğŸ”„ Comparing Different Recommendation Methods")
        print("=" * 70)
        
        # ä½¿ç”¨å°‘é‡æ ·æœ¬è¿›è¡Œæ¯”è¾ƒï¼ˆä¸ºäº†é€Ÿåº¦ï¼‰
        comparison_sample = available_papers[:10]
        comparison_results = evaluator.compare_methods(
            comparison_sample,
            methods=["collaborative", "content"]  # æš‚æ—¶åªæ¯”è¾ƒè¿™ä¸¤ç§æ–¹æ³•
        )
        
        # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
        if "comparison_report" in comparison_results:
            print("\nğŸ“Š Method Comparison Report:")
            for k, methods_data in comparison_results["comparison_report"].items():
                print(f"\n  {k}:")
                for method, metrics in methods_data.items():
                    print(f"    {method}:")
                    for metric, stats in metrics.items():
                        print(f"      {metric}: {stats['mean']:.4f} Â± {stats['std']:.4f}")
        
        # ä¿å­˜ç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ’¾ Saving Evaluation Results")
        print("=" * 70)
        
        all_results = {
            "evaluation_strategy": evaluator.evaluation_strategy,
            "data_analysis": data_analysis,
            "collaborative_results": collaborative_results,
            "comparison_results": comparison_results,
            "evaluation_config": {
                "k_values": evaluator.k_values,
                "test_samples": len(available_papers),
                "available_samples": len(available_papers),
                "evaluation_strategy": evaluator.evaluation_strategy,
                "timestamp": pd.Timestamp.now().isoformat()
            }
        }
        
        evaluator.save_results(all_results, "evaluation_results/evaluation_report.json")
        
        print("\nâœ… Evaluation completed successfully!")
        
        # æ˜¾ç¤ºç®€è¦æ€»ç»“
        print("\nğŸ“‹ Summary:")
        print(f"  Test coverage: {data_analysis['coverage']:.1%}")
        print(f"  Evaluation strategy: {evaluator.evaluation_strategy}")
        if "average_metrics" in collaborative_results:
            k = 10
            metrics = collaborative_results["average_metrics"].get(k, {})
            precision = metrics.get('precision', {}).get('mean', 0)
            recall = metrics.get('recall', {}).get('mean', 0)
            coverage = metrics.get('coverage', {}).get('mean', 1.0)
            print(f"  Precision@{k}: {precision:.4f}")
            print(f"  Recall@{k}: {recall:.4f}")
            print(f"  Ground truth coverage: {coverage:.1%}")
            print(f"  Successful evaluations: {collaborative_results.get('successful_evaluations', 0)}")
        
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()